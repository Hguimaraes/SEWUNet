{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import ConfigObject\n",
    "from utils import reserve_pop\n",
    "from utils import id_generator\n",
    "from utils import writer\n",
    "from utils import LibriSpeechGenerator\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from parts import VSConvBlock\n",
    "from parts import DownSamplingBlock\n",
    "from parts import UpSamplingBlock\n",
    "from parts import OutBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonConfig = {\n",
    "    \"test_platform\": False,\n",
    "    \"ds_prop\": 0.25,\n",
    "    \"sr\": 16000,\n",
    "    \"n_samples\": 65536,\n",
    "    \n",
    "    \"n_channels\": 1,\n",
    "    \"n_classes\": 1,\n",
    "    \"depth\": 5,\n",
    "    \"fsize\": 24,\n",
    "    \"moffset\": 8,\n",
    "    \n",
    "    \"batch_size\": 16,\n",
    "    \"epochs\": 25,\n",
    "    \"shuffle\": True,\n",
    "    \"num_workers\": 8,\n",
    "    \"verbose\": 100,\n",
    "\n",
    "    \"checkpoint_path\": \"../models/model_checkpoint.pt\",\n",
    "    \"model_path\": \"../models/last_model.pt\",\n",
    "\n",
    "    \"save_last_batch\": True,\n",
    "    \"writer_path\": \"../logs/\",\n",
    "    \"history_path\": \"../logs/history.json\"\n",
    "}\n",
    "\n",
    "config = ConfigObject(**jsonConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loaders\n",
    "_params = {\n",
    "    'batch_size': config.batch_size,\n",
    "    'shuffle': config.shuffle,\n",
    "    'num_workers': config.num_workers\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.load(\"../data/processed/noisy/train/x_train.pt\")\n",
    "y_train = torch.load(\"../data/processed/noisy/train/y_train.pt\")\n",
    "X_train_ae = torch.load(\"../data/processed/aewi/val/val.pt\")\n",
    "\n",
    "X_train = torch.cat([X_train, X_train_ae])\n",
    "y_train = torch.cat([y_train, X_train_ae])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "687"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del X_train_ae\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = torch.load(\"../data/processed/noisy/val/x_val.pt\")\n",
    "y_val = torch.load(\"../data/processed/noisy/val/y_val.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([36190, 1, 65536])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generators\n",
    "lsg = LibriSpeechGenerator(config, X_train, y_train)\n",
    "lsg_val = LibriSpeechGenerator(config, X_val, y_val)\n",
    "\n",
    "ls_generator = data.DataLoader(lsg, **_params)\n",
    "ls_val_generator = data.DataLoader(lsg_val, **_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEWUNet(nn.Module):\n",
    "    def __init__(self, config, fd=15, fu=5):\n",
    "        \"\"\"Speech Enhancenment using Wave-U-Net\"\"\"\n",
    "        super(SEWUNet, self).__init__()\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.n_channels = config.n_channels\n",
    "        self.n_classes = config.n_classes\n",
    "        self.depth = config.depth\n",
    "        self.fsize = config.fsize\n",
    "        self.moffset = config.moffset\n",
    "        self.fd = fd\n",
    "        self.fu = fu\n",
    "\n",
    "        # Generate the list of in, out channels for the encoder\n",
    "        self.enc_filters = [self.n_channels]\n",
    "        self.enc_filters += [self.fsize * i + self.moffset\n",
    "                             for i in range(1, self.depth + 1)]\n",
    "        self.n_encoder = zip(self.enc_filters, self.enc_filters[1:])\n",
    "\n",
    "        # Bottleneck block sizes\n",
    "        mid_in = self.fsize * self.depth + self.moffset\n",
    "        mid_out = self.fsize * (self.depth + 1) + self.moffset\n",
    "\n",
    "        # Generate the list of in, out channels for the decoder\n",
    "        self.out_dec = reserve_pop(self.enc_filters)\n",
    "        self.in_dec = [mid_out + self.enc_filters[-1]]\n",
    "        self.in_dec += [self.out_dec[i] + self.out_dec[i + 1]\n",
    "                        for i in range(self.depth - 1)]\n",
    "        self.n_decoder = zip(self.in_dec, self.out_dec)\n",
    "\n",
    "        # Architecture and parameters\n",
    "        self.encoder = nn.ModuleList()\n",
    "        self.decoder = nn.ModuleList()\n",
    "\n",
    "        # Build the encoder part of the U-net architecture\n",
    "        for i, (in_ch, out_ch) in enumerate(self.n_encoder):\n",
    "            self.encoder.append(DownSamplingBlock(\n",
    "                in_ch=in_ch,\n",
    "                out_ch=out_ch,\n",
    "                kernel_size=self.fd,\n",
    "                padding=self.fd // 2,\n",
    "                activation=nn.LeakyReLU(0.1))\n",
    "            )\n",
    "\n",
    "        # Bottleneck block for the U-net\n",
    "        self.mid_block = VSConvBlock(\n",
    "            in_ch=mid_in,\n",
    "            out_ch=mid_out,\n",
    "            kernel_size=self.fd,\n",
    "            padding=self.fd // 2,\n",
    "            activation=nn.LeakyReLU(0.1))\n",
    "\n",
    "        # Build the decoder part of the U-net architecture\n",
    "        for in_ch, out_ch in self.n_decoder:\n",
    "            self.decoder.append(UpSamplingBlock(\n",
    "                in_ch=in_ch,\n",
    "                out_ch=out_ch,\n",
    "                kernel_size=self.fu,\n",
    "                padding=self.fu // 2,\n",
    "                activation=nn.LeakyReLU(0.1))\n",
    "            )\n",
    "\n",
    "        # Output block\n",
    "        out_ch = self.out_dec[-1] + 1\n",
    "        self.out_block = OutBlock(\n",
    "            in_ch=out_ch,\n",
    "            out_ch=self.n_classes,\n",
    "            activation=nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\"\"\"\n",
    "        enc = []\n",
    "        net_in = copy.copy(x)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            x, xi = self.encoder[i](x)\n",
    "            enc.append(xi)\n",
    "\n",
    "        x = self.mid_block(x)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            x = self.decoder[i](x, enc.pop())\n",
    "\n",
    "        x = self.out_block(x, net_in)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "history = {'loss': [], 'SNR': [], 'val_loss': [], 'val_SNR': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SEWUNet(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomMetric():\n",
    "    \"\"\"Calculate the SNR of X and Y\"\"\"\n",
    "    def SNR(X, Y):\n",
    "        n = X.shape[2]\n",
    "        return torch.mean(10 * torch.log10(\n",
    "            (torch.norm(Y, dim=2)**2 / n) /\n",
    "            (torch.norm(X - Y, dim=2)**2 / n)\n",
    "        ))\n",
    "    return SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR (Validation): 10.05069351196289\n"
     ]
    }
   ],
   "source": [
    "# Build optimizer\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=3e-4,\n",
    "    weight_decay=1e-6,\n",
    "    betas=(0.9, 0.999))\n",
    "\n",
    "# lr_scheduler = torch.optim.lr_scheduler(optimizer)\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=1)\n",
    "\n",
    "# Loss and metric\n",
    "m_loss = nn.L1Loss()\n",
    "m_snr = CustomMetric()\n",
    "\n",
    "# Print validation metric before trainer\n",
    "print(\"SNR (Validation): {}\".format(m_snr(lsg_val.X, lsg_val.y).item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy model to device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('ae_checkpoint.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1139372"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the number of trainable parameters in the model\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "sum([np.prod(p.size()) for p in model_parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display trainning metrics\n",
    "def _display_metrics(epoch, it, steps, loss, metric):\n",
    "    print(\"Epoch [{:02d}/{:02d}]\".format(\n",
    "        epoch + 1, config.epochs), end=\", \")\n",
    "\n",
    "    print(\"Step [{:03d}/{:03d}]\".format(\n",
    "        it + 1, steps), end=\", \")\n",
    "\n",
    "    print(\"Loss: {}, SNR: {}\".format(\n",
    "        loss, metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [01/25], Step [100/2262], Loss: 0.010073499754071236, SNR: 13.109391212463379\n",
      "Epoch [01/25], Step [200/2262], Loss: 0.00986173003911972, SNR: 12.67269515991211\n",
      "Epoch [01/25], Step [300/2262], Loss: 0.022691166028380394, SNR: 11.14752197265625\n",
      "Epoch [01/25], Step [400/2262], Loss: 0.009030519053339958, SNR: 12.903619766235352\n",
      "Epoch [01/25], Step [500/2262], Loss: 0.010070573538541794, SNR: 12.481866836547852\n",
      "Epoch [01/25], Step [600/2262], Loss: 0.010999556630849838, SNR: 12.59660530090332\n",
      "Epoch [01/25], Step [700/2262], Loss: 0.010799899697303772, SNR: 11.850373268127441\n",
      "Epoch [01/25], Step [800/2262], Loss: 0.00789535604417324, SNR: 12.189760208129883\n",
      "Epoch [01/25], Step [900/2262], Loss: 0.010708009824156761, SNR: 10.975275993347168\n",
      "Epoch [01/25], Step [1000/2262], Loss: 0.010711289010941982, SNR: 12.268896102905273\n",
      "Epoch [01/25], Step [1100/2262], Loss: 0.007771109230816364, SNR: 13.66387939453125\n",
      "Epoch [01/25], Step [1200/2262], Loss: 0.008450979366898537, SNR: 14.233579635620117\n",
      "Epoch [01/25], Step [1300/2262], Loss: 0.009007357060909271, SNR: 13.378746032714844\n",
      "Epoch [01/25], Step [1400/2262], Loss: 0.009176718071103096, SNR: 12.778648376464844\n",
      "Epoch [01/25], Step [1500/2262], Loss: 0.008305570110678673, SNR: 13.207810401916504\n",
      "Epoch [01/25], Step [1600/2262], Loss: 0.008684560656547546, SNR: 13.442680358886719\n",
      "Epoch [01/25], Step [1700/2262], Loss: 0.008519341237843037, SNR: 12.894874572753906\n",
      "Epoch [01/25], Step [1800/2262], Loss: 0.008625730872154236, SNR: 13.90758991241455\n",
      "Epoch [01/25], Step [1900/2262], Loss: 0.007224850356578827, SNR: 14.179348945617676\n",
      "Epoch [01/25], Step [2000/2262], Loss: 0.009124738164246082, SNR: 13.857582092285156\n",
      "Epoch [01/25], Step [2100/2262], Loss: 0.007316255010664463, SNR: 13.886982917785645\n",
      "Epoch [01/25], Step [2200/2262], Loss: 0.009077521041035652, SNR: 15.769258499145508\n",
      ".:. Training metrics = Loss: 0.00978656037458154, SNR: 13.071747884199612\n",
      ".:. Validation metrics = Loss: 0.008869893762315787, SNR: 12.89269795628396\n",
      "Epoch [02/25], Step [100/2262], Loss: 0.007919903844594955, SNR: 13.751016616821289\n",
      "Epoch [02/25], Step [200/2262], Loss: 0.010490397922694683, SNR: 13.573827743530273\n",
      "Epoch [02/25], Step [300/2262], Loss: 0.006669568829238415, SNR: 14.913064956665039\n",
      "Epoch [02/25], Step [400/2262], Loss: 0.007484549656510353, SNR: 15.042343139648438\n",
      "Epoch [02/25], Step [500/2262], Loss: 0.007495208643376827, SNR: 14.72461223602295\n",
      "Epoch [02/25], Step [600/2262], Loss: 0.010688416659832, SNR: 12.346633911132812\n",
      "Epoch [02/25], Step [700/2262], Loss: 0.006943422835320234, SNR: 14.674175262451172\n",
      "Epoch [02/25], Step [800/2262], Loss: 0.009210568852722645, SNR: 15.852413177490234\n",
      "Epoch [02/25], Step [900/2262], Loss: 0.010530245490372181, SNR: 13.372151374816895\n",
      "Epoch [02/25], Step [1000/2262], Loss: 0.006741873454302549, SNR: 15.670344352722168\n",
      "Epoch [02/25], Step [1100/2262], Loss: 0.009650754742324352, SNR: 14.493118286132812\n",
      "Epoch [02/25], Step [1200/2262], Loss: 0.009648374281823635, SNR: 12.437068939208984\n",
      "Epoch [02/25], Step [1300/2262], Loss: 0.006141934543848038, SNR: 17.115571975708008\n",
      "Epoch [02/25], Step [1400/2262], Loss: 0.006589265074580908, SNR: 13.687318801879883\n",
      "Epoch [02/25], Step [1500/2262], Loss: 0.009136456996202469, SNR: 13.134963989257812\n",
      "Epoch [02/25], Step [1600/2262], Loss: 0.006428432650864124, SNR: 15.115941047668457\n",
      "Epoch [02/25], Step [1700/2262], Loss: 0.006272093392908573, SNR: 17.367874145507812\n",
      "Epoch [02/25], Step [1800/2262], Loss: 0.007380381226539612, SNR: 14.258049011230469\n",
      "Epoch [02/25], Step [1900/2262], Loss: 0.006193941924721003, SNR: 14.464046478271484\n",
      "Epoch [02/25], Step [2000/2262], Loss: 0.006743385456502438, SNR: 12.832258224487305\n",
      "Epoch [02/25], Step [2100/2262], Loss: 0.008699901401996613, SNR: 13.745115280151367\n",
      "Epoch [02/25], Step [2200/2262], Loss: 0.006492665037512779, SNR: 16.933074951171875\n",
      ".:. Training metrics = Loss: 0.007930264896274429, SNR: 14.670479064028576\n",
      ".:. Validation metrics = Loss: 0.00812455985588715, SNR: 13.572176335003602\n",
      "Epoch [03/25], Step [100/2262], Loss: 0.009153474122285843, SNR: 13.859757423400879\n",
      "Epoch [03/25], Step [200/2262], Loss: 0.007158578839153051, SNR: 14.568731307983398\n",
      "Epoch [03/25], Step [300/2262], Loss: 0.0044287219643592834, SNR: 16.394634246826172\n",
      "Epoch [03/25], Step [400/2262], Loss: 0.007673048414289951, SNR: 15.953473091125488\n",
      "Epoch [03/25], Step [500/2262], Loss: 0.0066310022957623005, SNR: 15.74252700805664\n",
      "Epoch [03/25], Step [600/2262], Loss: 0.0056144120171666145, SNR: 16.183557510375977\n",
      "Epoch [03/25], Step [700/2262], Loss: 0.009395863860845566, SNR: 15.59279727935791\n",
      "Epoch [03/25], Step [800/2262], Loss: 0.007116382941603661, SNR: 16.360013961791992\n",
      "Epoch [03/25], Step [900/2262], Loss: 0.008486486971378326, SNR: 13.988771438598633\n",
      "Epoch [03/25], Step [1000/2262], Loss: 0.004825095180422068, SNR: 18.674259185791016\n",
      "Epoch [03/25], Step [1100/2262], Loss: 0.007076821755617857, SNR: 15.157537460327148\n",
      "Epoch [03/25], Step [1200/2262], Loss: 0.008754383772611618, SNR: 15.374192237854004\n",
      "Epoch [03/25], Step [1300/2262], Loss: 0.007323507219552994, SNR: 14.59505844116211\n",
      "Epoch [03/25], Step [1400/2262], Loss: 0.008754635229706764, SNR: 13.967897415161133\n",
      "Epoch [03/25], Step [1500/2262], Loss: 0.009345883503556252, SNR: 14.666675567626953\n",
      "Epoch [03/25], Step [1600/2262], Loss: 0.00894269160926342, SNR: 17.196754455566406\n",
      "Epoch [03/25], Step [1700/2262], Loss: 0.006898073945194483, SNR: 13.71931266784668\n",
      "Epoch [03/25], Step [1800/2262], Loss: 0.006468895357102156, SNR: 17.295440673828125\n",
      "Epoch [03/25], Step [1900/2262], Loss: 0.005060246214270592, SNR: 17.54500961303711\n",
      "Epoch [03/25], Step [2000/2262], Loss: 0.007007650099694729, SNR: 15.679403305053711\n",
      "Epoch [03/25], Step [2100/2262], Loss: 0.006665017455816269, SNR: 14.827253341674805\n",
      "Epoch [03/25], Step [2200/2262], Loss: 0.007062126882374287, SNR: 16.673036575317383\n",
      ".:. Training metrics = Loss: 0.007306603755380904, SNR: 15.335271232712497\n",
      ".:. Validation metrics = Loss: 0.007577820623274306, SNR: 14.109407858254574\n",
      "Epoch [04/25], Step [100/2262], Loss: 0.008351856842637062, SNR: 15.784431457519531\n",
      "Epoch [04/25], Step [200/2262], Loss: 0.008043434470891953, SNR: 14.683870315551758\n",
      "Epoch [04/25], Step [300/2262], Loss: 0.008311426267027855, SNR: 14.14181137084961\n",
      "Epoch [04/25], Step [400/2262], Loss: 0.008374117314815521, SNR: 14.897605895996094\n",
      "Epoch [04/25], Step [500/2262], Loss: 0.007577058859169483, SNR: 16.254432678222656\n",
      "Epoch [04/25], Step [600/2262], Loss: 0.006322737783193588, SNR: 13.746682167053223\n",
      "Epoch [04/25], Step [700/2262], Loss: 0.0055626449175179005, SNR: 16.652782440185547\n",
      "Epoch [04/25], Step [800/2262], Loss: 0.006455357186496258, SNR: 15.813163757324219\n",
      "Epoch [04/25], Step [900/2262], Loss: 0.00860375352203846, SNR: 15.162487983703613\n",
      "Epoch [04/25], Step [1000/2262], Loss: 0.007347201928496361, SNR: 14.342081069946289\n",
      "Epoch [04/25], Step [1100/2262], Loss: 0.006519203074276447, SNR: 16.441219329833984\n",
      "Epoch [04/25], Step [1200/2262], Loss: 0.00807565450668335, SNR: 15.213083267211914\n",
      "Epoch [04/25], Step [1300/2262], Loss: 0.009517417289316654, SNR: 14.10491943359375\n",
      "Epoch [04/25], Step [1400/2262], Loss: 0.0046396683901548386, SNR: 18.149890899658203\n",
      "Epoch [04/25], Step [1500/2262], Loss: 0.008173169568181038, SNR: 16.9722900390625\n",
      "Epoch [04/25], Step [1600/2262], Loss: 0.004918468650430441, SNR: 18.104568481445312\n",
      "Epoch [04/25], Step [1700/2262], Loss: 0.005249431356787682, SNR: 16.438766479492188\n",
      "Epoch [04/25], Step [1800/2262], Loss: 0.0078347809612751, SNR: 14.958256721496582\n",
      "Epoch [04/25], Step [1900/2262], Loss: 0.006636566482484341, SNR: 16.539989471435547\n",
      "Epoch [04/25], Step [2000/2262], Loss: 0.006946241483092308, SNR: 15.079849243164062\n",
      "Epoch [04/25], Step [2100/2262], Loss: 0.005348621867597103, SNR: 17.760597229003906\n",
      "Epoch [04/25], Step [2200/2262], Loss: 0.007639991585165262, SNR: 14.970029830932617\n",
      ".:. Training metrics = Loss: 0.0069759837818301395, SNR: 15.741061633685307\n",
      ".:. Validation metrics = Loss: 0.007579064631183858, SNR: 14.116684856379482\n",
      "Epoch [05/25], Step [100/2262], Loss: 0.008659256622195244, SNR: 14.637697219848633\n",
      "Epoch [05/25], Step [200/2262], Loss: 0.00668312655761838, SNR: 16.359943389892578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [05/25], Step [300/2262], Loss: 0.006699901074171066, SNR: 16.129615783691406\n",
      "Epoch [05/25], Step [400/2262], Loss: 0.005522642284631729, SNR: 18.157299041748047\n",
      "Epoch [05/25], Step [500/2262], Loss: 0.007050966843962669, SNR: 15.01240348815918\n",
      "Epoch [05/25], Step [600/2262], Loss: 0.005889163352549076, SNR: 17.474308013916016\n",
      "Epoch [05/25], Step [700/2262], Loss: 0.00830416101962328, SNR: 15.535910606384277\n",
      "Epoch [05/25], Step [800/2262], Loss: 0.008572682738304138, SNR: 13.882553100585938\n",
      "Epoch [05/25], Step [900/2262], Loss: 0.006524287164211273, SNR: 15.850003242492676\n",
      "Epoch [05/25], Step [1000/2262], Loss: 0.00660133920609951, SNR: 14.624847412109375\n",
      "Epoch [05/25], Step [1100/2262], Loss: 0.005637154448777437, SNR: 16.440654754638672\n",
      "Epoch [05/25], Step [1200/2262], Loss: 0.006839856971055269, SNR: 18.389530181884766\n",
      "Epoch [05/25], Step [1300/2262], Loss: 0.005625124555081129, SNR: 16.06655502319336\n",
      "Epoch [05/25], Step [1400/2262], Loss: 0.0059187086299061775, SNR: 18.453567504882812\n",
      "Epoch [05/25], Step [1500/2262], Loss: 0.01066408958286047, SNR: 16.84343719482422\n",
      "Epoch [05/25], Step [1600/2262], Loss: 0.005628293380141258, SNR: 15.956040382385254\n",
      "Epoch [05/25], Step [1700/2262], Loss: 0.005872227717190981, SNR: 17.80747413635254\n",
      "Epoch [05/25], Step [1800/2262], Loss: 0.00648969691246748, SNR: 18.322307586669922\n",
      "Epoch [05/25], Step [1900/2262], Loss: 0.005222827196121216, SNR: 17.176639556884766\n",
      "Epoch [05/25], Step [2000/2262], Loss: 0.008477035909891129, SNR: 15.554581642150879\n",
      "Epoch [05/25], Step [2100/2262], Loss: 0.00605923542752862, SNR: 15.876927375793457\n",
      "Epoch [05/25], Step [2200/2262], Loss: 0.00532239954918623, SNR: 16.81413459777832\n",
      ".:. Training metrics = Loss: 0.006754045417822092, SNR: 16.03688529394845\n",
      ".:. Validation metrics = Loss: 0.007404474033394036, SNR: 14.286997372370742\n",
      "Epoch [06/25], Step [100/2262], Loss: 0.007293363101780415, SNR: 14.36677360534668\n",
      "Epoch [06/25], Step [200/2262], Loss: 0.008383107371628284, SNR: 13.416072845458984\n",
      "Epoch [06/25], Step [300/2262], Loss: 0.006566104479134083, SNR: 14.754732131958008\n",
      "Epoch [06/25], Step [400/2262], Loss: 0.009570790454745293, SNR: 15.046638488769531\n",
      "Epoch [06/25], Step [500/2262], Loss: 0.006108570843935013, SNR: 15.227469444274902\n",
      "Epoch [06/25], Step [600/2262], Loss: 0.006108494009822607, SNR: 15.548684120178223\n",
      "Epoch [06/25], Step [700/2262], Loss: 0.006403283216059208, SNR: 17.349489212036133\n",
      "Epoch [06/25], Step [800/2262], Loss: 0.0057721808552742004, SNR: 18.178638458251953\n",
      "Epoch [06/25], Step [900/2262], Loss: 0.0049180155619978905, SNR: 17.224212646484375\n",
      "Epoch [06/25], Step [1000/2262], Loss: 0.008569232188165188, SNR: 14.926514625549316\n",
      "Epoch [06/25], Step [1100/2262], Loss: 0.004894803278148174, SNR: 17.46816062927246\n",
      "Epoch [06/25], Step [1200/2262], Loss: 0.008998939767479897, SNR: 15.582226753234863\n",
      "Epoch [06/25], Step [1300/2262], Loss: 0.008703131228685379, SNR: 17.059749603271484\n",
      "Epoch [06/25], Step [1400/2262], Loss: 0.007056599948555231, SNR: 16.016334533691406\n",
      "Epoch [06/25], Step [1500/2262], Loss: 0.004989259410649538, SNR: 16.977630615234375\n",
      "Epoch [06/25], Step [1600/2262], Loss: 0.006980043835937977, SNR: 15.84411334991455\n",
      "Epoch [06/25], Step [1700/2262], Loss: 0.006246422417461872, SNR: 15.907217025756836\n",
      "Epoch [06/25], Step [1800/2262], Loss: 0.00893666036427021, SNR: 16.051876068115234\n",
      "Epoch [06/25], Step [1900/2262], Loss: 0.0053093163296580315, SNR: 16.44692611694336\n",
      "Epoch [06/25], Step [2000/2262], Loss: 0.005624862387776375, SNR: 16.164676666259766\n",
      "Epoch [06/25], Step [2100/2262], Loss: 0.006246217526495457, SNR: 16.250154495239258\n",
      "Epoch [06/25], Step [2200/2262], Loss: 0.007210295647382736, SNR: 14.68736743927002\n",
      ".:. Training metrics = Loss: 0.0066099472776257094, SNR: 16.24981047753373\n",
      ".:. Validation metrics = Loss: 0.007201483790806813, SNR: 14.492821801121794\n",
      "Epoch [07/25], Step [100/2262], Loss: 0.011053529568016529, SNR: 13.985494613647461\n",
      "Epoch [07/25], Step [200/2262], Loss: 0.00594004150480032, SNR: 17.641117095947266\n",
      "Epoch [07/25], Step [300/2262], Loss: 0.006986851803958416, SNR: 18.910263061523438\n",
      "Epoch [07/25], Step [400/2262], Loss: 0.008327345363795757, SNR: 16.48338508605957\n",
      "Epoch [07/25], Step [500/2262], Loss: 0.006131255533546209, SNR: 17.056541442871094\n",
      "Epoch [07/25], Step [600/2262], Loss: 0.006487706676125526, SNR: 15.918859481811523\n",
      "Epoch [07/25], Step [700/2262], Loss: 0.0048006074503064156, SNR: 18.897329330444336\n",
      "Epoch [07/25], Step [800/2262], Loss: 0.005163492169231176, SNR: 16.63207244873047\n",
      "Epoch [07/25], Step [900/2262], Loss: 0.00971660204231739, SNR: 13.273681640625\n",
      "Epoch [07/25], Step [1000/2262], Loss: 0.005211669020354748, SNR: 18.36170196533203\n",
      "Epoch [07/25], Step [1100/2262], Loss: 0.0065259006805717945, SNR: 14.892087936401367\n",
      "Epoch [07/25], Step [1200/2262], Loss: 0.005600328557193279, SNR: 16.929439544677734\n",
      "Epoch [07/25], Step [1300/2262], Loss: 0.007690745871514082, SNR: 14.906782150268555\n",
      "Epoch [07/25], Step [1400/2262], Loss: 0.006820401176810265, SNR: 17.016990661621094\n",
      "Epoch [07/25], Step [1500/2262], Loss: 0.006229661405086517, SNR: 17.05227279663086\n",
      "Epoch [07/25], Step [1600/2262], Loss: 0.00484093464910984, SNR: 17.89923667907715\n",
      "Epoch [07/25], Step [1700/2262], Loss: 0.007882237434387207, SNR: 14.511281967163086\n",
      "Epoch [07/25], Step [1800/2262], Loss: 0.005065797362476587, SNR: 15.286334991455078\n",
      "Epoch [07/25], Step [1900/2262], Loss: 0.006383756175637245, SNR: 15.857288360595703\n",
      "Epoch [07/25], Step [2000/2262], Loss: 0.006192835047841072, SNR: 16.530372619628906\n",
      "Epoch [07/25], Step [2100/2262], Loss: 0.0040842099115252495, SNR: 19.160133361816406\n",
      "Epoch [07/25], Step [2200/2262], Loss: 0.005177336744964123, SNR: 16.014020919799805\n",
      ".:. Training metrics = Loss: 0.006490125692531603, SNR: 16.419354554782686\n",
      ".:. Validation metrics = Loss: 0.0072333040167876835, SNR: 14.547515645918788\n",
      "Epoch [08/25], Step [100/2262], Loss: 0.005908983759582043, SNR: 17.29092025756836\n",
      "Epoch [08/25], Step [200/2262], Loss: 0.0076350015588104725, SNR: 15.688146591186523\n",
      "Epoch [08/25], Step [300/2262], Loss: 0.005295534152537584, SNR: 15.822139739990234\n",
      "Epoch [08/25], Step [400/2262], Loss: 0.011910341680049896, SNR: 14.34636116027832\n",
      "Epoch [08/25], Step [500/2262], Loss: 0.005788000766187906, SNR: 17.66884422302246\n",
      "Epoch [08/25], Step [600/2262], Loss: 0.006681780330836773, SNR: 16.663978576660156\n",
      "Epoch [08/25], Step [700/2262], Loss: 0.004796529188752174, SNR: 18.554697036743164\n",
      "Epoch [08/25], Step [800/2262], Loss: 0.006620417349040508, SNR: 16.908336639404297\n",
      "Epoch [08/25], Step [900/2262], Loss: 0.006499613169580698, SNR: 15.401015281677246\n",
      "Epoch [08/25], Step [1000/2262], Loss: 0.005277812946587801, SNR: 18.13941764831543\n",
      "Epoch [08/25], Step [1100/2262], Loss: 0.00572029035538435, SNR: 17.997215270996094\n",
      "Epoch [08/25], Step [1200/2262], Loss: 0.007422030903398991, SNR: 16.141586303710938\n",
      "Epoch [08/25], Step [1300/2262], Loss: 0.006190164014697075, SNR: 17.89649200439453\n",
      "Epoch [08/25], Step [1400/2262], Loss: 0.006539320573210716, SNR: 15.05500602722168\n",
      "Epoch [08/25], Step [1500/2262], Loss: 0.004792108666151762, SNR: 16.95030975341797\n",
      "Epoch [08/25], Step [1600/2262], Loss: 0.006170075386762619, SNR: 15.377100944519043\n",
      "Epoch [08/25], Step [1700/2262], Loss: 0.00746874138712883, SNR: 15.881950378417969\n",
      "Epoch [08/25], Step [1800/2262], Loss: 0.004934178665280342, SNR: 19.258819580078125\n",
      "Epoch [08/25], Step [1900/2262], Loss: 0.006958766840398312, SNR: 15.621861457824707\n",
      "Epoch [08/25], Step [2000/2262], Loss: 0.0073806168511509895, SNR: 17.26884651184082\n",
      "Epoch [08/25], Step [2100/2262], Loss: 0.00630048057064414, SNR: 14.986444473266602\n",
      "Epoch [08/25], Step [2200/2262], Loss: 0.006361614912748337, SNR: 17.798091888427734\n",
      ".:. Training metrics = Loss: 0.0063775414000845606, SNR: 16.609433693107107\n",
      ".:. Validation metrics = Loss: 0.007110358589284242, SNR: 14.67380677490315\n",
      "Epoch [09/25], Step [100/2262], Loss: 0.005738301668316126, SNR: 14.544450759887695\n",
      "Epoch [09/25], Step [200/2262], Loss: 0.005381168331950903, SNR: 18.128448486328125\n",
      "Epoch [09/25], Step [300/2262], Loss: 0.005411696620285511, SNR: 18.018035888671875\n",
      "Epoch [09/25], Step [400/2262], Loss: 0.007174930535256863, SNR: 17.216033935546875\n",
      "Epoch [09/25], Step [500/2262], Loss: 0.007254426367580891, SNR: 15.545431137084961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [09/25], Step [600/2262], Loss: 0.007341567426919937, SNR: 16.994487762451172\n",
      "Epoch [09/25], Step [700/2262], Loss: 0.00582544133067131, SNR: 17.071685791015625\n",
      "Epoch [09/25], Step [800/2262], Loss: 0.0060255806893110275, SNR: 15.844731330871582\n",
      "Epoch [09/25], Step [900/2262], Loss: 0.005589403212070465, SNR: 18.758686065673828\n",
      "Epoch [09/25], Step [1000/2262], Loss: 0.006331928074359894, SNR: 15.54908275604248\n",
      "Epoch [09/25], Step [1100/2262], Loss: 0.007758882828056812, SNR: 17.465368270874023\n",
      "Epoch [09/25], Step [1200/2262], Loss: 0.006142079830169678, SNR: 16.317689895629883\n",
      "Epoch [09/25], Step [1300/2262], Loss: 0.006093543488532305, SNR: 15.429410934448242\n",
      "Epoch [09/25], Step [1400/2262], Loss: 0.0050511229783296585, SNR: 17.637983322143555\n",
      "Epoch [09/25], Step [1500/2262], Loss: 0.007165201939642429, SNR: 14.48163890838623\n",
      "Epoch [09/25], Step [1600/2262], Loss: 0.004602330271154642, SNR: 19.954710006713867\n",
      "Epoch [09/25], Step [1700/2262], Loss: 0.00665480550378561, SNR: 15.631536483764648\n",
      "Epoch [09/25], Step [1800/2262], Loss: 0.004849021323025227, SNR: 17.856611251831055\n",
      "Epoch [09/25], Step [1900/2262], Loss: 0.005328081082552671, SNR: 17.363693237304688\n",
      "Epoch [09/25], Step [2000/2262], Loss: 0.006425574421882629, SNR: 15.69243049621582\n",
      "Epoch [09/25], Step [2100/2262], Loss: 0.005641341209411621, SNR: 17.519033432006836\n",
      "Epoch [09/25], Step [2200/2262], Loss: 0.00559263676404953, SNR: 14.889580726623535\n",
      ".:. Training metrics = Loss: 0.006295388734931114, SNR: 16.7471682162468\n",
      ".:. Validation metrics = Loss: 0.007039854512371357, SNR: 14.75227428623937\n",
      "Epoch [10/25], Step [100/2262], Loss: 0.006901109591126442, SNR: 15.550359725952148\n",
      "Epoch [10/25], Step [200/2262], Loss: 0.0054329209960997105, SNR: 17.920257568359375\n",
      "Epoch [10/25], Step [300/2262], Loss: 0.006023022346198559, SNR: 19.09250831604004\n",
      "Epoch [10/25], Step [400/2262], Loss: 0.005410779733210802, SNR: 18.83765411376953\n",
      "Epoch [10/25], Step [500/2262], Loss: 0.006256444379687309, SNR: 15.661651611328125\n",
      "Epoch [10/25], Step [600/2262], Loss: 0.006710279732942581, SNR: 15.074363708496094\n",
      "Epoch [10/25], Step [700/2262], Loss: 0.006505854427814484, SNR: 16.803903579711914\n",
      "Epoch [10/25], Step [800/2262], Loss: 0.006262597162276506, SNR: 15.301568031311035\n",
      "Epoch [10/25], Step [900/2262], Loss: 0.005818103905767202, SNR: 18.690658569335938\n",
      "Epoch [10/25], Step [1000/2262], Loss: 0.008249959908425808, SNR: 14.560064315795898\n",
      "Epoch [10/25], Step [1100/2262], Loss: 0.0066916970536112785, SNR: 15.269430160522461\n",
      "Epoch [10/25], Step [1200/2262], Loss: 0.006082400679588318, SNR: 16.16710662841797\n",
      "Epoch [10/25], Step [1300/2262], Loss: 0.006123943254351616, SNR: 15.599920272827148\n",
      "Epoch [10/25], Step [1400/2262], Loss: 0.006307919509708881, SNR: 18.400959014892578\n",
      "Epoch [10/25], Step [1500/2262], Loss: 0.007674063090234995, SNR: 16.7757511138916\n",
      "Epoch [10/25], Step [1600/2262], Loss: 0.008509606122970581, SNR: 15.79499626159668\n",
      "Epoch [10/25], Step [1700/2262], Loss: 0.00758736114948988, SNR: 15.034618377685547\n",
      "Epoch [10/25], Step [1800/2262], Loss: 0.005573006346821785, SNR: 17.261810302734375\n",
      "Epoch [10/25], Step [1900/2262], Loss: 0.006490339525043964, SNR: 16.449764251708984\n",
      "Epoch [10/25], Step [2000/2262], Loss: 0.004831066355109215, SNR: 17.320804595947266\n",
      "Epoch [10/25], Step [2100/2262], Loss: 0.005630091764032841, SNR: 17.424875259399414\n",
      "Epoch [10/25], Step [2200/2262], Loss: 0.0047347755171358585, SNR: 17.402538299560547\n",
      ".:. Training metrics = Loss: 0.006219356451991613, SNR: 16.88760521997566\n",
      ".:. Validation metrics = Loss: 0.006907636729551588, SNR: 14.877167688605333\n",
      "Epoch [11/25], Step [100/2262], Loss: 0.007852159440517426, SNR: 15.113142013549805\n",
      "Epoch [11/25], Step [200/2262], Loss: 0.006462867371737957, SNR: 16.947498321533203\n",
      "Epoch [11/25], Step [300/2262], Loss: 0.00529654324054718, SNR: 18.55035400390625\n",
      "Epoch [11/25], Step [400/2262], Loss: 0.0062805102206766605, SNR: 16.824893951416016\n",
      "Epoch [11/25], Step [500/2262], Loss: 0.006497148424386978, SNR: 16.136009216308594\n",
      "Epoch [11/25], Step [600/2262], Loss: 0.011699394322931767, SNR: 13.40310287475586\n",
      "Epoch [11/25], Step [700/2262], Loss: 0.007709586061537266, SNR: 15.724493026733398\n",
      "Epoch [11/25], Step [800/2262], Loss: 0.006207563448697329, SNR: 16.112442016601562\n",
      "Epoch [11/25], Step [900/2262], Loss: 0.0077635738998651505, SNR: 17.360637664794922\n",
      "Epoch [11/25], Step [1000/2262], Loss: 0.006378149148076773, SNR: 15.551250457763672\n",
      "Epoch [11/25], Step [1100/2262], Loss: 0.0062285917811095715, SNR: 16.38193130493164\n",
      "Epoch [11/25], Step [1200/2262], Loss: 0.005564326420426369, SNR: 17.285720825195312\n",
      "Epoch [11/25], Step [1300/2262], Loss: 0.008733823895454407, SNR: 15.703727722167969\n",
      "Epoch [11/25], Step [1400/2262], Loss: 0.006881866604089737, SNR: 15.835653305053711\n",
      "Epoch [11/25], Step [1500/2262], Loss: 0.006281668320298195, SNR: 17.113853454589844\n",
      "Epoch [11/25], Step [1600/2262], Loss: 0.006902859080582857, SNR: 16.377439498901367\n",
      "Epoch [11/25], Step [1700/2262], Loss: 0.006485768593847752, SNR: 15.100834846496582\n",
      "Epoch [11/25], Step [1800/2262], Loss: 0.00574534572660923, SNR: 17.49818992614746\n",
      "Epoch [11/25], Step [1900/2262], Loss: 0.006126082502305508, SNR: 16.97172737121582\n",
      "Epoch [11/25], Step [2000/2262], Loss: 0.006388339214026928, SNR: 16.387348175048828\n",
      "Epoch [11/25], Step [2100/2262], Loss: 0.004557314328849316, SNR: 19.502687454223633\n",
      "Epoch [11/25], Step [2200/2262], Loss: 0.006493058521300554, SNR: 18.79006004333496\n",
      ".:. Training metrics = Loss: 0.006160084980340022, SNR: 16.98991347189337\n",
      ".:. Validation metrics = Loss: 0.00684167078618109, SNR: 14.954311671001008\n",
      "Epoch [12/25], Step [100/2262], Loss: 0.0068798307329416275, SNR: 14.093744277954102\n",
      "Epoch [12/25], Step [200/2262], Loss: 0.004461349919438362, SNR: 18.191085815429688\n",
      "Epoch [12/25], Step [300/2262], Loss: 0.0050962637178599834, SNR: 18.142892837524414\n",
      "Epoch [12/25], Step [400/2262], Loss: 0.007965873926877975, SNR: 15.16494369506836\n",
      "Epoch [12/25], Step [500/2262], Loss: 0.007355164736509323, SNR: 14.437010765075684\n",
      "Epoch [12/25], Step [600/2262], Loss: 0.006660222075879574, SNR: 14.968550682067871\n",
      "Epoch [12/25], Step [700/2262], Loss: 0.0052226269617676735, SNR: 17.471561431884766\n",
      "Epoch [12/25], Step [800/2262], Loss: 0.008367771282792091, SNR: 17.67903709411621\n",
      "Epoch [12/25], Step [900/2262], Loss: 0.005360843148082495, SNR: 15.92626953125\n",
      "Epoch [12/25], Step [1000/2262], Loss: 0.005745262838900089, SNR: 17.238143920898438\n",
      "Epoch [12/25], Step [1100/2262], Loss: 0.007865987718105316, SNR: 16.75681495666504\n",
      "Epoch [12/25], Step [1200/2262], Loss: 0.006589171476662159, SNR: 16.131324768066406\n",
      "Epoch [12/25], Step [1300/2262], Loss: 0.0053788754157722, SNR: 18.69660758972168\n",
      "Epoch [12/25], Step [1400/2262], Loss: 0.007356266025453806, SNR: 17.95063018798828\n",
      "Epoch [12/25], Step [1500/2262], Loss: 0.005713941529393196, SNR: 17.17876434326172\n",
      "Epoch [12/25], Step [1600/2262], Loss: 0.006322021596133709, SNR: 17.148679733276367\n",
      "Epoch [12/25], Step [1700/2262], Loss: 0.005443612579256296, SNR: 15.010879516601562\n",
      "Epoch [12/25], Step [1800/2262], Loss: 0.007537505589425564, SNR: 14.649463653564453\n",
      "Epoch [12/25], Step [1900/2262], Loss: 0.007430818397551775, SNR: 15.411959648132324\n",
      "Epoch [12/25], Step [2000/2262], Loss: 0.005897107068449259, SNR: 16.527774810791016\n",
      "Epoch [12/25], Step [2100/2262], Loss: 0.005624338984489441, SNR: 19.015453338623047\n",
      "Epoch [12/25], Step [2200/2262], Loss: 0.010825985111296177, SNR: 15.861908912658691\n",
      ".:. Training metrics = Loss: 0.006121397106495227, SNR: 17.076004287938975\n",
      ".:. Validation metrics = Loss: 0.006791579956201114, SNR: 15.059834529974573\n",
      "Epoch [13/25], Step [100/2262], Loss: 0.0047617508098483086, SNR: 19.116750717163086\n",
      "Epoch [13/25], Step [200/2262], Loss: 0.004110366106033325, SNR: 19.130943298339844\n",
      "Epoch [13/25], Step [300/2262], Loss: 0.005846881773322821, SNR: 19.344928741455078\n",
      "Epoch [13/25], Step [400/2262], Loss: 0.006102291867136955, SNR: 17.800941467285156\n",
      "Epoch [13/25], Step [500/2262], Loss: 0.0055655306205153465, SNR: 16.99983787536621\n",
      "Epoch [13/25], Step [600/2262], Loss: 0.009014543145895004, SNR: 15.399273872375488\n",
      "Epoch [13/25], Step [700/2262], Loss: 0.00548841105774045, SNR: 18.3707275390625\n",
      "Epoch [13/25], Step [800/2262], Loss: 0.006613053847104311, SNR: 17.14511489868164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/25], Step [900/2262], Loss: 0.006610979791730642, SNR: 18.649799346923828\n",
      "Epoch [13/25], Step [1000/2262], Loss: 0.006600207183510065, SNR: 18.255889892578125\n",
      "Epoch [13/25], Step [1100/2262], Loss: 0.005626536440104246, SNR: 16.754438400268555\n",
      "Epoch [13/25], Step [1200/2262], Loss: 0.00457048648968339, SNR: 18.250078201293945\n",
      "Epoch [13/25], Step [1300/2262], Loss: 0.0052406685426831245, SNR: 16.890504837036133\n",
      "Epoch [13/25], Step [1400/2262], Loss: 0.006856178864836693, SNR: 15.482704162597656\n",
      "Epoch [13/25], Step [1500/2262], Loss: 0.0055589680559933186, SNR: 15.428620338439941\n",
      "Epoch [13/25], Step [1600/2262], Loss: 0.004875805228948593, SNR: 18.4979248046875\n",
      "Epoch [13/25], Step [1700/2262], Loss: 0.005078540183603764, SNR: 16.741992950439453\n",
      "Epoch [13/25], Step [1800/2262], Loss: 0.0066548120230436325, SNR: 15.815057754516602\n",
      "Epoch [13/25], Step [1900/2262], Loss: 0.004501631483435631, SNR: 18.65723419189453\n",
      "Epoch [13/25], Step [2000/2262], Loss: 0.006561798043549061, SNR: 16.163738250732422\n",
      "Epoch [13/25], Step [2100/2262], Loss: 0.007680687587708235, SNR: 14.882205963134766\n",
      "Epoch [13/25], Step [2200/2262], Loss: 0.006584391463547945, SNR: 18.892990112304688\n",
      ".:. Training metrics = Loss: 0.006068026839103627, SNR: 17.16422713944582\n",
      ".:. Validation metrics = Loss: 0.006804418152913006, SNR: 14.97329021792852\n",
      "Epoch [14/25], Step [100/2262], Loss: 0.005693077575415373, SNR: 16.70699691772461\n",
      "Epoch [14/25], Step [200/2262], Loss: 0.006327763199806213, SNR: 16.094425201416016\n",
      "Epoch [14/25], Step [300/2262], Loss: 0.004562198184430599, SNR: 17.846036911010742\n",
      "Epoch [14/25], Step [400/2262], Loss: 0.006260789465159178, SNR: 16.878660202026367\n",
      "Epoch [14/25], Step [500/2262], Loss: 0.006204318255186081, SNR: 17.468181610107422\n",
      "Epoch [14/25], Step [600/2262], Loss: 0.005493366625159979, SNR: 16.461458206176758\n",
      "Epoch [14/25], Step [700/2262], Loss: 0.0054247151128947735, SNR: 16.384368896484375\n",
      "Epoch [14/25], Step [800/2262], Loss: 0.005519537720829248, SNR: 17.362520217895508\n",
      "Epoch [14/25], Step [900/2262], Loss: 0.006546132266521454, SNR: 18.290586471557617\n",
      "Epoch [14/25], Step [1000/2262], Loss: 0.005870998837053776, SNR: 17.33349609375\n",
      "Epoch [14/25], Step [1100/2262], Loss: 0.007237695623189211, SNR: 17.549285888671875\n",
      "Epoch [14/25], Step [1200/2262], Loss: 0.007058665622025728, SNR: 14.658857345581055\n",
      "Epoch [14/25], Step [1300/2262], Loss: 0.00606546550989151, SNR: 15.720854759216309\n",
      "Epoch [14/25], Step [1400/2262], Loss: 0.008910729549825191, SNR: 16.007137298583984\n",
      "Epoch [14/25], Step [1500/2262], Loss: 0.0061132051050662994, SNR: 17.68273162841797\n",
      "Epoch [14/25], Step [1600/2262], Loss: 0.0048313750885427, SNR: 17.571460723876953\n",
      "Epoch [14/25], Step [1700/2262], Loss: 0.006152379792183638, SNR: 16.233333587646484\n",
      "Epoch [14/25], Step [1800/2262], Loss: 0.006922988221049309, SNR: 16.7265625\n",
      "Epoch [14/25], Step [1900/2262], Loss: 0.008766504935920238, SNR: 15.548707008361816\n",
      "Epoch [14/25], Step [2000/2262], Loss: 0.005454195197671652, SNR: 17.565399169921875\n",
      "Epoch [14/25], Step [2100/2262], Loss: 0.0058793192729353905, SNR: 18.14237403869629\n",
      "Epoch [14/25], Step [2200/2262], Loss: 0.004452101420611143, SNR: 20.630830764770508\n",
      ".:. Training metrics = Loss: 0.006012399123740208, SNR: 17.277272787987446\n",
      ".:. Validation metrics = Loss: 0.006977010336284582, SNR: 14.902346571423143\n",
      "Epoch [15/25], Step [100/2262], Loss: 0.0059037539176642895, SNR: 17.872249603271484\n",
      "Epoch [15/25], Step [200/2262], Loss: 0.0065812463872134686, SNR: 18.173065185546875\n",
      "Epoch [15/25], Step [300/2262], Loss: 0.005716855637729168, SNR: 16.907459259033203\n",
      "Epoch [15/25], Step [400/2262], Loss: 0.004713336005806923, SNR: 16.723026275634766\n",
      "Epoch [15/25], Step [500/2262], Loss: 0.004161742515861988, SNR: 21.012195587158203\n",
      "Epoch [15/25], Step [600/2262], Loss: 0.0050577204674482346, SNR: 16.783905029296875\n",
      "Epoch [15/25], Step [700/2262], Loss: 0.007189163006842136, SNR: 16.171829223632812\n",
      "Epoch [15/25], Step [800/2262], Loss: 0.004616341087967157, SNR: 21.318042755126953\n",
      "Epoch [15/25], Step [900/2262], Loss: 0.005711887963116169, SNR: 17.31854248046875\n",
      "Epoch [15/25], Step [1000/2262], Loss: 0.0042000277899205685, SNR: 21.087711334228516\n",
      "Epoch [15/25], Step [1100/2262], Loss: 0.005811943672597408, SNR: 16.271791458129883\n",
      "Epoch [15/25], Step [1200/2262], Loss: 0.004993785172700882, SNR: 19.592466354370117\n",
      "Epoch [15/25], Step [1300/2262], Loss: 0.0054706791415810585, SNR: 18.827850341796875\n",
      "Epoch [15/25], Step [1400/2262], Loss: 0.004485333804041147, SNR: 18.761768341064453\n",
      "Epoch [15/25], Step [1500/2262], Loss: 0.004227303434163332, SNR: 19.30986785888672\n",
      "Epoch [15/25], Step [1600/2262], Loss: 0.0047769080847501755, SNR: 21.442737579345703\n",
      "Epoch [15/25], Step [1700/2262], Loss: 0.004443190060555935, SNR: 19.518123626708984\n",
      "Epoch [15/25], Step [1800/2262], Loss: 0.005067393183708191, SNR: 18.032066345214844\n",
      "Epoch [15/25], Step [1900/2262], Loss: 0.006152568385004997, SNR: 15.139742851257324\n",
      "Epoch [15/25], Step [2000/2262], Loss: 0.003942769020795822, SNR: 20.966297149658203\n",
      "Epoch [15/25], Step [2100/2262], Loss: 0.004728537052869797, SNR: 18.665618896484375\n",
      "Epoch [15/25], Step [2200/2262], Loss: 0.004583730362355709, SNR: 18.62499237060547\n",
      ".:. Training metrics = Loss: 0.005578882737382711, SNR: 18.065022964983495\n",
      ".:. Validation metrics = Loss: 0.006484372586014842, SNR: 15.429906007695596\n",
      "Epoch [16/25], Step [100/2262], Loss: 0.005413622595369816, SNR: 15.605672836303711\n",
      "Epoch [16/25], Step [200/2262], Loss: 0.004038750194013119, SNR: 20.96062469482422\n",
      "Epoch [16/25], Step [300/2262], Loss: 0.005506257526576519, SNR: 18.42494773864746\n",
      "Epoch [16/25], Step [400/2262], Loss: 0.004156817682087421, SNR: 19.827342987060547\n",
      "Epoch [16/25], Step [500/2262], Loss: 0.004660776816308498, SNR: 20.708202362060547\n",
      "Epoch [16/25], Step [600/2262], Loss: 0.005672125145792961, SNR: 16.979080200195312\n",
      "Epoch [16/25], Step [700/2262], Loss: 0.007166908122599125, SNR: 17.008785247802734\n",
      "Epoch [16/25], Step [800/2262], Loss: 0.004365835338830948, SNR: 20.159643173217773\n",
      "Epoch [16/25], Step [900/2262], Loss: 0.004589086398482323, SNR: 18.65247917175293\n",
      "Epoch [16/25], Step [1000/2262], Loss: 0.005716540850698948, SNR: 17.663253784179688\n",
      "Epoch [16/25], Step [1100/2262], Loss: 0.0058492994867265224, SNR: 18.13852310180664\n",
      "Epoch [16/25], Step [1200/2262], Loss: 0.0042733922600746155, SNR: 19.54026985168457\n",
      "Epoch [16/25], Step [1300/2262], Loss: 0.006381453014910221, SNR: 19.31744384765625\n",
      "Epoch [16/25], Step [1400/2262], Loss: 0.00598439946770668, SNR: 17.031049728393555\n",
      "Epoch [16/25], Step [1500/2262], Loss: 0.005245465785264969, SNR: 19.93181800842285\n",
      "Epoch [16/25], Step [1600/2262], Loss: 0.009429247118532658, SNR: 16.632755279541016\n",
      "Epoch [16/25], Step [1700/2262], Loss: 0.006658896803855896, SNR: 17.94387435913086\n",
      "Epoch [16/25], Step [1800/2262], Loss: 0.005035314708948135, SNR: 18.085050582885742\n",
      "Epoch [16/25], Step [1900/2262], Loss: 0.005912273656576872, SNR: 16.296018600463867\n",
      "Epoch [16/25], Step [2000/2262], Loss: 0.0081177968531847, SNR: 18.692546844482422\n",
      "Epoch [16/25], Step [2100/2262], Loss: 0.00846895482391119, SNR: 15.054583549499512\n",
      "Epoch [16/25], Step [2200/2262], Loss: 0.0034728622995316982, SNR: 20.665922164916992\n",
      ".:. Training metrics = Loss: 0.005524609213204436, SNR: 18.174790817770916\n",
      ".:. Validation metrics = Loss: 0.006494201034385316, SNR: 15.445468091926887\n",
      "Epoch [17/25], Step [100/2262], Loss: 0.009614390321075916, SNR: 17.811033248901367\n",
      "Epoch [17/25], Step [200/2262], Loss: 0.005097830668091774, SNR: 16.84501838684082\n",
      "Epoch [17/25], Step [300/2262], Loss: 0.006656639277935028, SNR: 17.500045776367188\n",
      "Epoch [17/25], Step [400/2262], Loss: 0.0055715483613312244, SNR: 17.478681564331055\n",
      "Epoch [17/25], Step [500/2262], Loss: 0.005180829204618931, SNR: 19.81612777709961\n",
      "Epoch [17/25], Step [600/2262], Loss: 0.004960039164870977, SNR: 17.106889724731445\n",
      "Epoch [17/25], Step [700/2262], Loss: 0.005193824879825115, SNR: 17.87377166748047\n",
      "Epoch [17/25], Step [800/2262], Loss: 0.003758316859602928, SNR: 20.439624786376953\n",
      "Epoch [17/25], Step [900/2262], Loss: 0.004824160598218441, SNR: 19.685993194580078\n",
      "Epoch [17/25], Step [1000/2262], Loss: 0.0038907548878341913, SNR: 20.04395294189453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/25], Step [1100/2262], Loss: 0.00692654587328434, SNR: 17.748554229736328\n",
      "Epoch [17/25], Step [1200/2262], Loss: 0.005021425895392895, SNR: 17.661378860473633\n",
      "Epoch [17/25], Step [1300/2262], Loss: 0.005376808345317841, SNR: 17.63542366027832\n",
      "Epoch [17/25], Step [1400/2262], Loss: 0.004745339974761009, SNR: 18.74362564086914\n",
      "Epoch [17/25], Step [1500/2262], Loss: 0.0045410883612930775, SNR: 19.625911712646484\n",
      "Epoch [17/25], Step [1600/2262], Loss: 0.006494068540632725, SNR: 16.195545196533203\n",
      "Epoch [17/25], Step [1700/2262], Loss: 0.004622194916009903, SNR: 17.232704162597656\n",
      "Epoch [17/25], Step [1800/2262], Loss: 0.0059006474912166595, SNR: 19.284696578979492\n",
      "Epoch [17/25], Step [1900/2262], Loss: 0.006908899173140526, SNR: 16.168956756591797\n",
      "Epoch [17/25], Step [2000/2262], Loss: 0.005187513306736946, SNR: 17.732669830322266\n",
      "Epoch [17/25], Step [2100/2262], Loss: 0.005443971138447523, SNR: 17.757640838623047\n",
      "Epoch [17/25], Step [2200/2262], Loss: 0.005663905758410692, SNR: 15.740747451782227\n",
      ".:. Training metrics = Loss: 0.005496838355051422, SNR: 18.23386593186483\n",
      ".:. Validation metrics = Loss: 0.006505123376664039, SNR: 15.436598593140053\n",
      "Epoch [18/25], Step [100/2262], Loss: 0.0042977770790457726, SNR: 18.305870056152344\n",
      "Epoch [18/25], Step [200/2262], Loss: 0.005413677077740431, SNR: 15.869670867919922\n",
      "Epoch [18/25], Step [300/2262], Loss: 0.005220766179263592, SNR: 17.029644012451172\n",
      "Epoch [18/25], Step [400/2262], Loss: 0.006154005415737629, SNR: 18.594738006591797\n",
      "Epoch [18/25], Step [500/2262], Loss: 0.004758851602673531, SNR: 19.851625442504883\n",
      "Epoch [18/25], Step [600/2262], Loss: 0.004977515898644924, SNR: 17.741077423095703\n",
      "Epoch [18/25], Step [700/2262], Loss: 0.005350811406970024, SNR: 19.113739013671875\n",
      "Epoch [18/25], Step [800/2262], Loss: 0.004801744595170021, SNR: 17.317562103271484\n",
      "Epoch [18/25], Step [900/2262], Loss: 0.006413767114281654, SNR: 17.595447540283203\n",
      "Epoch [18/25], Step [1000/2262], Loss: 0.004488575272262096, SNR: 18.17430877685547\n",
      "Epoch [18/25], Step [1100/2262], Loss: 0.00655718008056283, SNR: 15.813661575317383\n",
      "Epoch [18/25], Step [1200/2262], Loss: 0.0039830636233091354, SNR: 21.618228912353516\n",
      "Epoch [18/25], Step [1300/2262], Loss: 0.005052623338997364, SNR: 19.83399200439453\n",
      "Epoch [18/25], Step [1400/2262], Loss: 0.004486006684601307, SNR: 19.96231460571289\n",
      "Epoch [18/25], Step [1500/2262], Loss: 0.005500032100826502, SNR: 15.933111190795898\n",
      "Epoch [18/25], Step [1600/2262], Loss: 0.00605264725163579, SNR: 15.773508071899414\n",
      "Epoch [18/25], Step [1700/2262], Loss: 0.0051343608647584915, SNR: 17.700531005859375\n",
      "Epoch [18/25], Step [1800/2262], Loss: 0.0032898043282330036, SNR: 23.039264678955078\n",
      "Epoch [18/25], Step [1900/2262], Loss: 0.008109768852591515, SNR: 16.678939819335938\n",
      "Epoch [18/25], Step [2000/2262], Loss: 0.00630408339202404, SNR: 16.577688217163086\n",
      "Epoch [18/25], Step [2100/2262], Loss: 0.005406673531979322, SNR: 19.203630447387695\n",
      "Epoch [18/25], Step [2200/2262], Loss: 0.005875376518815756, SNR: 18.15911102294922\n",
      ".:. Training metrics = Loss: 0.005443361397324638, SNR: 18.334680742768168\n",
      ".:. Validation metrics = Loss: 0.0064401853802773775, SNR: 15.50722888915257\n",
      "Epoch [19/25], Step [100/2262], Loss: 0.004145841114223003, SNR: 18.952457427978516\n",
      "Epoch [19/25], Step [200/2262], Loss: 0.005594973918050528, SNR: 19.40936851501465\n",
      "Epoch [19/25], Step [300/2262], Loss: 0.007293026894330978, SNR: 18.38947105407715\n",
      "Epoch [19/25], Step [400/2262], Loss: 0.004819848574697971, SNR: 20.260169982910156\n",
      "Epoch [19/25], Step [500/2262], Loss: 0.006033786106854677, SNR: 18.371252059936523\n",
      "Epoch [19/25], Step [600/2262], Loss: 0.00465728621929884, SNR: 18.218576431274414\n",
      "Epoch [19/25], Step [700/2262], Loss: 0.005997371859848499, SNR: 17.590431213378906\n",
      "Epoch [19/25], Step [800/2262], Loss: 0.005588081199675798, SNR: 17.485498428344727\n",
      "Epoch [19/25], Step [900/2262], Loss: 0.005355377681553364, SNR: 18.501230239868164\n",
      "Epoch [19/25], Step [1000/2262], Loss: 0.005048468708992004, SNR: 16.9903621673584\n",
      "Epoch [19/25], Step [1100/2262], Loss: 0.006749770138412714, SNR: 17.03987693786621\n",
      "Epoch [19/25], Step [1200/2262], Loss: 0.005439586006104946, SNR: 19.457807540893555\n",
      "Epoch [19/25], Step [1300/2262], Loss: 0.007318247575312853, SNR: 15.720105171203613\n",
      "Epoch [19/25], Step [1400/2262], Loss: 0.004360358230769634, SNR: 19.036319732666016\n",
      "Epoch [19/25], Step [1500/2262], Loss: 0.004316684789955616, SNR: 18.944665908813477\n",
      "Epoch [19/25], Step [1600/2262], Loss: 0.00564172537997365, SNR: 20.13723373413086\n",
      "Epoch [19/25], Step [1700/2262], Loss: 0.0062688253819942474, SNR: 17.847633361816406\n",
      "Epoch [19/25], Step [1800/2262], Loss: 0.006425884552299976, SNR: 19.269926071166992\n",
      "Epoch [19/25], Step [1900/2262], Loss: 0.00598263181746006, SNR: 17.481918334960938\n",
      "Epoch [19/25], Step [2000/2262], Loss: 0.004065894056111574, SNR: 19.71473503112793\n",
      "Epoch [19/25], Step [2100/2262], Loss: 0.0051134200766682625, SNR: 17.271318435668945\n",
      "Epoch [19/25], Step [2200/2262], Loss: 0.0053982604295015335, SNR: 17.769515991210938\n",
      ".:. Training metrics = Loss: 0.005437830713857814, SNR: 18.35002675811439\n",
      ".:. Validation metrics = Loss: 0.006436203700273291, SNR: 15.51510747694143\n",
      "Epoch [20/25], Step [100/2262], Loss: 0.004452425055205822, SNR: 20.921937942504883\n",
      "Epoch [20/25], Step [200/2262], Loss: 0.00717639084905386, SNR: 16.41033935546875\n",
      "Epoch [20/25], Step [300/2262], Loss: 0.004967980086803436, SNR: 17.513336181640625\n",
      "Epoch [20/25], Step [400/2262], Loss: 0.005666794721037149, SNR: 17.323074340820312\n",
      "Epoch [20/25], Step [500/2262], Loss: 0.0036862895358353853, SNR: 22.33958625793457\n",
      "Epoch [20/25], Step [600/2262], Loss: 0.0054468633607029915, SNR: 16.263940811157227\n",
      "Epoch [20/25], Step [700/2262], Loss: 0.003653543069958687, SNR: 22.176986694335938\n",
      "Epoch [20/25], Step [800/2262], Loss: 0.004265414550900459, SNR: 19.746719360351562\n",
      "Epoch [20/25], Step [900/2262], Loss: 0.006120698060840368, SNR: 17.046977996826172\n",
      "Epoch [20/25], Step [1000/2262], Loss: 0.005003783851861954, SNR: 18.163619995117188\n",
      "Epoch [20/25], Step [1100/2262], Loss: 0.005155191756784916, SNR: 19.217247009277344\n",
      "Epoch [20/25], Step [1200/2262], Loss: 0.004850002937018871, SNR: 18.943592071533203\n",
      "Epoch [20/25], Step [1300/2262], Loss: 0.005387584678828716, SNR: 18.33443260192871\n",
      "Epoch [20/25], Step [1400/2262], Loss: 0.006576398387551308, SNR: 19.174341201782227\n",
      "Epoch [20/25], Step [1500/2262], Loss: 0.004761548712849617, SNR: 17.668426513671875\n",
      "Epoch [20/25], Step [1600/2262], Loss: 0.005877020303159952, SNR: 17.313011169433594\n",
      "Epoch [20/25], Step [1700/2262], Loss: 0.004486262798309326, SNR: 18.936519622802734\n",
      "Epoch [20/25], Step [1800/2262], Loss: 0.005168539937585592, SNR: 17.87225914001465\n",
      "Epoch [20/25], Step [1900/2262], Loss: 0.005460122600197792, SNR: 17.741703033447266\n",
      "Epoch [20/25], Step [2000/2262], Loss: 0.003577697090804577, SNR: 20.82231903076172\n",
      "Epoch [20/25], Step [2100/2262], Loss: 0.006792370229959488, SNR: 19.046905517578125\n",
      "Epoch [20/25], Step [2200/2262], Loss: 0.006171846762299538, SNR: 16.393033981323242\n",
      ".:. Training metrics = Loss: 0.005434311836758234, SNR: 18.35859031972387\n",
      ".:. Validation metrics = Loss: 0.00643368513332973, SNR: 15.517138312399561\n",
      "Epoch [21/25], Step [100/2262], Loss: 0.0048917364329099655, SNR: 20.42376708984375\n",
      "Epoch [21/25], Step [200/2262], Loss: 0.005610615015029907, SNR: 17.197673797607422\n",
      "Epoch [21/25], Step [300/2262], Loss: 0.005924921482801437, SNR: 18.654953002929688\n",
      "Epoch [21/25], Step [400/2262], Loss: 0.004212929401546717, SNR: 18.825143814086914\n",
      "Epoch [21/25], Step [500/2262], Loss: 0.002683158265426755, SNR: 23.400848388671875\n",
      "Epoch [21/25], Step [600/2262], Loss: 0.0047377608716487885, SNR: 18.53689193725586\n",
      "Epoch [21/25], Step [700/2262], Loss: 0.005700100213289261, SNR: 18.959747314453125\n",
      "Epoch [21/25], Step [800/2262], Loss: 0.00380472419783473, SNR: 19.991369247436523\n",
      "Epoch [21/25], Step [900/2262], Loss: 0.004050837829709053, SNR: 20.184310913085938\n",
      "Epoch [21/25], Step [1000/2262], Loss: 0.004248581361025572, SNR: 19.709197998046875\n",
      "Epoch [21/25], Step [1100/2262], Loss: 0.005655257496982813, SNR: 17.28792381286621\n",
      "Epoch [21/25], Step [1200/2262], Loss: 0.00484125129878521, SNR: 20.22607421875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/25], Step [1300/2262], Loss: 0.005319399759173393, SNR: 20.11300277709961\n",
      "Epoch [21/25], Step [1400/2262], Loss: 0.004453950561583042, SNR: 20.915912628173828\n",
      "Epoch [21/25], Step [1500/2262], Loss: 0.00525689497590065, SNR: 20.0122127532959\n",
      "Epoch [21/25], Step [1600/2262], Loss: 0.006981964688748121, SNR: 16.411548614501953\n",
      "Epoch [21/25], Step [1700/2262], Loss: 0.0028606238774955273, SNR: 20.950136184692383\n",
      "Epoch [21/25], Step [1800/2262], Loss: 0.006278203800320625, SNR: 17.336807250976562\n",
      "Epoch [21/25], Step [1900/2262], Loss: 0.004423447418957949, SNR: 17.218463897705078\n",
      "Epoch [21/25], Step [2000/2262], Loss: 0.005362465046346188, SNR: 17.88842010498047\n",
      "Epoch [21/25], Step [2100/2262], Loss: 0.004278725013136864, SNR: 18.799531936645508\n",
      "Epoch [21/25], Step [2200/2262], Loss: 0.006455130409449339, SNR: 18.49901580810547\n",
      ".:. Training metrics = Loss: 0.005431437176199587, SNR: 18.364933255088367\n",
      ".:. Validation metrics = Loss: 0.006442160742306632, SNR: 15.51281491703093\n",
      "Epoch [22/25], Step [100/2262], Loss: 0.003264126367866993, SNR: 23.573671340942383\n",
      "Epoch [22/25], Step [200/2262], Loss: 0.0049837371334433556, SNR: 16.481266021728516\n",
      "Epoch [22/25], Step [300/2262], Loss: 0.006460181437432766, SNR: 16.01704978942871\n",
      "Epoch [22/25], Step [400/2262], Loss: 0.0068677375093102455, SNR: 15.762088775634766\n",
      "Epoch [22/25], Step [500/2262], Loss: 0.0044555384665727615, SNR: 19.168445587158203\n",
      "Epoch [22/25], Step [600/2262], Loss: 0.0060339635238051414, SNR: 15.355730056762695\n",
      "Epoch [22/25], Step [700/2262], Loss: 0.0053762574680149555, SNR: 17.805137634277344\n",
      "Epoch [22/25], Step [800/2262], Loss: 0.004703053273260593, SNR: 19.16079330444336\n",
      "Epoch [22/25], Step [900/2262], Loss: 0.009112872183322906, SNR: 18.689350128173828\n",
      "Epoch [22/25], Step [1000/2262], Loss: 0.006082679610699415, SNR: 18.192626953125\n",
      "Epoch [22/25], Step [1100/2262], Loss: 0.004620484076440334, SNR: 18.592769622802734\n",
      "Epoch [22/25], Step [1200/2262], Loss: 0.0036489921621978283, SNR: 21.711071014404297\n",
      "Epoch [22/25], Step [1300/2262], Loss: 0.0032096270006150007, SNR: 22.26384735107422\n",
      "Epoch [22/25], Step [1400/2262], Loss: 0.004719275515526533, SNR: 18.436351776123047\n",
      "Epoch [22/25], Step [1500/2262], Loss: 0.005078644026070833, SNR: 17.148561477661133\n",
      "Epoch [22/25], Step [1600/2262], Loss: 0.00595677737146616, SNR: 19.036792755126953\n",
      "Epoch [22/25], Step [1700/2262], Loss: 0.009725562296807766, SNR: 15.135992050170898\n",
      "Epoch [22/25], Step [1800/2262], Loss: 0.005999117158353329, SNR: 17.63416862487793\n",
      "Epoch [22/25], Step [1900/2262], Loss: 0.00513567216694355, SNR: 17.28881072998047\n",
      "Epoch [22/25], Step [2000/2262], Loss: 0.004756244830787182, SNR: 20.09622573852539\n",
      "Epoch [22/25], Step [2100/2262], Loss: 0.0069872597232460976, SNR: 15.991283416748047\n",
      "Epoch [22/25], Step [2200/2262], Loss: 0.0050901263020932674, SNR: 18.29380989074707\n",
      ".:. Training metrics = Loss: 0.005428633000212715, SNR: 18.370519824843605\n",
      ".:. Validation metrics = Loss: 0.006436358060524226, SNR: 15.51312978139664\n",
      "Epoch [23/25], Step [100/2262], Loss: 0.004699519369751215, SNR: 19.956533432006836\n",
      "Epoch [23/25], Step [200/2262], Loss: 0.002822406589984894, SNR: 21.31068229675293\n",
      "Epoch [23/25], Step [300/2262], Loss: 0.003174122888594866, SNR: 23.29582977294922\n",
      "Epoch [23/25], Step [400/2262], Loss: 0.0029579296242445707, SNR: 21.20565414428711\n",
      "Epoch [23/25], Step [500/2262], Loss: 0.0041330549865961075, SNR: 19.328096389770508\n",
      "Epoch [23/25], Step [600/2262], Loss: 0.005683382973074913, SNR: 19.06929588317871\n",
      "Epoch [23/25], Step [700/2262], Loss: 0.006721732206642628, SNR: 16.177167892456055\n",
      "Epoch [23/25], Step [800/2262], Loss: 0.0033141174353659153, SNR: 20.87820053100586\n",
      "Epoch [23/25], Step [900/2262], Loss: 0.006133873015642166, SNR: 17.211830139160156\n",
      "Epoch [23/25], Step [1000/2262], Loss: 0.0068346671760082245, SNR: 15.20564079284668\n",
      "Epoch [23/25], Step [1100/2262], Loss: 0.004899214021861553, SNR: 18.13075065612793\n",
      "Epoch [23/25], Step [1200/2262], Loss: 0.00726573821157217, SNR: 15.33318042755127\n",
      "Epoch [23/25], Step [1300/2262], Loss: 0.004895055200904608, SNR: 18.77060317993164\n",
      "Epoch [23/25], Step [1400/2262], Loss: 0.005162178538739681, SNR: 15.973773002624512\n",
      "Epoch [23/25], Step [1500/2262], Loss: 0.005076969973742962, SNR: 18.955875396728516\n",
      "Epoch [23/25], Step [1600/2262], Loss: 0.0052222865633666515, SNR: 16.121742248535156\n",
      "Epoch [23/25], Step [1700/2262], Loss: 0.005493415053933859, SNR: 17.019712448120117\n",
      "Epoch [23/25], Step [1800/2262], Loss: 0.004167397506535053, SNR: 20.783203125\n",
      "Epoch [23/25], Step [1900/2262], Loss: 0.0038586074952036142, SNR: 21.4637451171875\n",
      "Epoch [23/25], Step [2000/2262], Loss: 0.004525443073362112, SNR: 17.91254997253418\n",
      "Epoch [23/25], Step [2100/2262], Loss: 0.005014025606215, SNR: 19.493045806884766\n",
      "Epoch [23/25], Step [2200/2262], Loss: 0.007451272569596767, SNR: 17.30580711364746\n",
      ".:. Training metrics = Loss: 0.005422900732103287, SNR: 18.38482162916189\n",
      ".:. Validation metrics = Loss: 0.006431804446484307, SNR: 15.521662351663261\n",
      "Epoch [24/25], Step [100/2262], Loss: 0.005205212160944939, SNR: 17.081226348876953\n",
      "Epoch [24/25], Step [200/2262], Loss: 0.0048882607370615005, SNR: 18.971256256103516\n",
      "Epoch [24/25], Step [300/2262], Loss: 0.0036430289037525654, SNR: 19.58389663696289\n",
      "Epoch [24/25], Step [400/2262], Loss: 0.006400765385478735, SNR: 18.271574020385742\n",
      "Epoch [24/25], Step [500/2262], Loss: 0.005578699056059122, SNR: 18.622634887695312\n",
      "Epoch [24/25], Step [600/2262], Loss: 0.0043304250575602055, SNR: 17.7890567779541\n",
      "Epoch [24/25], Step [700/2262], Loss: 0.005328690633177757, SNR: 16.85921287536621\n",
      "Epoch [24/25], Step [800/2262], Loss: 0.005183418281376362, SNR: 17.68036651611328\n",
      "Epoch [24/25], Step [900/2262], Loss: 0.00561918318271637, SNR: 18.3124942779541\n",
      "Epoch [24/25], Step [1000/2262], Loss: 0.004681752994656563, SNR: 18.355682373046875\n",
      "Epoch [24/25], Step [1100/2262], Loss: 0.005139364395290613, SNR: 17.584415435791016\n",
      "Epoch [24/25], Step [1200/2262], Loss: 0.005771107040345669, SNR: 18.056074142456055\n",
      "Epoch [24/25], Step [1300/2262], Loss: 0.002873407443985343, SNR: 24.005107879638672\n",
      "Epoch [24/25], Step [1400/2262], Loss: 0.00556559395045042, SNR: 17.658037185668945\n",
      "Epoch [24/25], Step [1500/2262], Loss: 0.005386265926063061, SNR: 17.033653259277344\n",
      "Epoch [24/25], Step [1600/2262], Loss: 0.005313237197697163, SNR: 17.582014083862305\n",
      "Epoch [24/25], Step [1700/2262], Loss: 0.008917272090911865, SNR: 15.115346908569336\n",
      "Epoch [24/25], Step [1800/2262], Loss: 0.00522134592756629, SNR: 18.330615997314453\n",
      "Epoch [24/25], Step [1900/2262], Loss: 0.004546491429209709, SNR: 18.907405853271484\n",
      "Epoch [24/25], Step [2000/2262], Loss: 0.004562773276120424, SNR: 18.44769287109375\n",
      "Epoch [24/25], Step [2100/2262], Loss: 0.004748987033963203, SNR: 18.927885055541992\n",
      "Epoch [24/25], Step [2200/2262], Loss: 0.00560825364664197, SNR: 17.560020446777344\n",
      ".:. Training metrics = Loss: 0.005422148445678263, SNR: 18.38675006582909\n",
      ".:. Validation metrics = Loss: 0.006430597397129558, SNR: 15.52407622494984\n",
      "Epoch [25/25], Step [100/2262], Loss: 0.004045548848807812, SNR: 18.976757049560547\n",
      "Epoch [25/25], Step [200/2262], Loss: 0.005712688900530338, SNR: 19.167987823486328\n",
      "Epoch [25/25], Step [300/2262], Loss: 0.008399570360779762, SNR: 13.878782272338867\n",
      "Epoch [25/25], Step [400/2262], Loss: 0.0074053057469427586, SNR: 15.867561340332031\n",
      "Epoch [25/25], Step [500/2262], Loss: 0.003705377224832773, SNR: 18.549327850341797\n",
      "Epoch [25/25], Step [600/2262], Loss: 0.0067346468567848206, SNR: 17.00703239440918\n",
      "Epoch [25/25], Step [700/2262], Loss: 0.005322606302797794, SNR: 18.322811126708984\n",
      "Epoch [25/25], Step [800/2262], Loss: 0.005467432551085949, SNR: 19.10767936706543\n",
      "Epoch [25/25], Step [900/2262], Loss: 0.006394560448825359, SNR: 17.188983917236328\n",
      "Epoch [25/25], Step [1000/2262], Loss: 0.005728541407734156, SNR: 17.05708885192871\n",
      "Epoch [25/25], Step [1100/2262], Loss: 0.0052916863933205605, SNR: 16.5445556640625\n",
      "Epoch [25/25], Step [1200/2262], Loss: 0.0024717035703361034, SNR: 24.478031158447266\n",
      "Epoch [25/25], Step [1300/2262], Loss: 0.004080299753695726, SNR: 20.78963279724121\n",
      "Epoch [25/25], Step [1400/2262], Loss: 0.005788198672235012, SNR: 18.983356475830078\n",
      "Epoch [25/25], Step [1500/2262], Loss: 0.004922464489936829, SNR: 17.97888946533203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/25], Step [1600/2262], Loss: 0.003858492709696293, SNR: 19.056407928466797\n",
      "Epoch [25/25], Step [1700/2262], Loss: 0.006728010252118111, SNR: 18.0033016204834\n",
      "Epoch [25/25], Step [1800/2262], Loss: 0.0047414484433829784, SNR: 16.2210693359375\n",
      "Epoch [25/25], Step [1900/2262], Loss: 0.004302086308598518, SNR: 19.2296199798584\n",
      "Epoch [25/25], Step [2000/2262], Loss: 0.004512303974479437, SNR: 18.33623695373535\n",
      "Epoch [25/25], Step [2100/2262], Loss: 0.005882935598492622, SNR: 18.77545166015625\n",
      "Epoch [25/25], Step [2200/2262], Loss: 0.0051514324732124805, SNR: 17.896934509277344\n",
      ".:. Training metrics = Loss: 0.005421813032284902, SNR: 18.382419522831736\n",
      ".:. Validation metrics = Loss: 0.006433615310985129, SNR: 15.519936502310877\n"
     ]
    }
   ],
   "source": [
    "# Train the model over epochs\n",
    "steps = len(ls_generator)\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "    # training and val metrics for all data\n",
    "    loss, metric = 0.0, 0.0\n",
    "    val_loss, val_metric = 0.0, 0.0\n",
    "\n",
    "    # ======================== Training ============================= #\n",
    "    for i, (local_batch, local_labels) in enumerate(ls_generator):\n",
    "        # Transfer to Device\n",
    "        local_batch = local_batch.to(device)\n",
    "        local_labels = local_labels.to(device)\n",
    "\n",
    "        # Set gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass, backward pass, optimize\n",
    "        outputs = model(local_batch)\n",
    "        loss_batch = m_loss(outputs, local_labels)\n",
    "        batch_metric = m_snr(outputs, local_labels)\n",
    "        loss_batch.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute metrics to all batch\n",
    "        loss += loss_batch.item() * len(local_batch)\n",
    "        metric += batch_metric.item() * len(local_batch)\n",
    "\n",
    "        # Print the loss every \"verbose\" batches\n",
    "        if (i + 1) % config.verbose == 0:\n",
    "            _display_metrics(epoch, i, steps,\n",
    "                loss_batch.item(), batch_metric.item())\n",
    "\n",
    "    # Compute the statistics of the last epoch and save to history\n",
    "    history['loss'].append(loss / len(lsg))\n",
    "    history['SNR'].append(metric / len(lsg))\n",
    "\n",
    "    # Checkpoint the model\n",
    "    torch.save(model.state_dict(), config.checkpoint_path)\n",
    "    \n",
    "    # Print Validation statistics\n",
    "    print(\".:. Training metrics =\", end=\" \")\n",
    "    print(\"Loss: {}, SNR: {}\".format(loss / len(lsg), metric / len(lsg)))\n",
    "    \n",
    "    # ======================= Validation ============================ #\n",
    "    with torch.no_grad():\n",
    "        for local_batch, local_labels in ls_val_generator:\n",
    "            # Transfer to device\n",
    "            local_batch = local_batch.to(device)\n",
    "            local_labels = local_labels.to(device)\n",
    "\n",
    "            # Predict, get loss and metric\n",
    "            outputs = model(local_batch)\n",
    "            val_loss += m_loss(outputs, local_labels).item() \\\n",
    "                * len(local_batch)\n",
    "\n",
    "            val_metric += m_snr(outputs, local_labels).item() \\\n",
    "                * len(local_batch)\n",
    "\n",
    "        val_loss /= len(lsg_val)\n",
    "        val_metric /= len(lsg_val)\n",
    "                \n",
    "    # Print Validation statistics\n",
    "    print(\".:. Validation metrics =\", end=\" \")\n",
    "    print(\"Loss: {}, SNR: {}\".format(val_loss, val_metric))\n",
    "\n",
    "    # Compute the metrics and loss of last batch and save to history\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_SNR'].append(val_metric)\n",
    "    lr_scheduler.step(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the last model\n",
    "torch.save(model.state_dict(), config.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot network history\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.plot(history['loss'], label='train')\n",
    "plt.plot(history['val_loss'], label='val')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Traning history')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot network history\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.plot(history['SNR'], label='train')\n",
    "plt.plot(history['val_SNR'], label='val')\n",
    "plt.ylabel('SNR')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Traning history')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save history to a JSON file\n",
    "with open(config.history_path, 'w') as fp:\n",
    "    json.dump(history, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.load(\"../data/processed/noisy/test/x_test.pt\")\n",
    "y_test = torch.load(\"../data/processed/noisy/test/y_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsg_test = LibriSpeechGenerator(config, X_test, y_test)\n",
    "ls_test_generator = data.DataLoader(lsg_test, **_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3874, 1, 65536])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR (Test): 10.058753967285156\n"
     ]
    }
   ],
   "source": [
    "# Print validation metric before trainer\n",
    "print(\"SNR (Test): {}\".format(m_snr(lsg_test.X, lsg_test.y).item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_metric = 0.0, 0.0\n",
    "with torch.no_grad():\n",
    "    for local_batch, local_labels in ls_test_generator:\n",
    "        # Transfer to device\n",
    "        local_batch = local_batch.to(device)\n",
    "        local_labels = local_labels.to(device)\n",
    "\n",
    "        # Predict, get loss and metric\n",
    "        outputs = model(local_batch)\n",
    "        test_loss += m_loss(outputs, local_labels).item() \\\n",
    "            * len(local_batch)\n",
    "\n",
    "        test_metric += m_snr(outputs, local_labels).item() \\\n",
    "            * len(local_batch)\n",
    "        \n",
    "        writer(local_batch, local_labels,\n",
    "               outputs, config.sr, config.writer_path)\n",
    "\n",
    "    test_loss /= len(lsg_test)\n",
    "    test_metric /= len(lsg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "test_history = {\n",
    "    'SNR_ini': m_snr(lsg_test.X, lsg_test.y).item(),\n",
    "    'SNR': test_metric, \n",
    "    'loss': test_loss\n",
    "}\n",
    "\n",
    "# with open(os.path.join(config.writer_path, 'test_history.json') , 'w') as fp:\n",
    "#     json.dump(test_history, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SNR_ini': 10.058753967285156,\n",
       " 'SNR': 15.73345841214284,\n",
       " 'loss': 0.006422024996731013}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.4 s, sys: 1.25 s, total: 22.6 s\n",
      "Wall time: 23.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    for local_batch, local_labels in ls_test_generator:\n",
    "        # Transfer to device\n",
    "        local_batch = local_batch.to(device)\n",
    "        local_labels = local_labels.to(device)\n",
    "\n",
    "        # Predict, get loss and metric\n",
    "        outputs = model(local_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3874, 1, 65536])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005962829117191534"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "23.1/3874"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37min 43s, sys: 5min 35s, total: 43min 18s\n",
      "Wall time: 17min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with torch.no_grad():\n",
    "    for local_batch, local_labels in ls_test_generator:\n",
    "        # Transfer to device\n",
    "        local_batch = local_batch.to(\"cpu\")\n",
    "        local_labels = local_labels.to(\"cpu\")\n",
    "\n",
    "        # Predict, get loss and metric\n",
    "        outputs = model(local_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/3874"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sewunet",
   "language": "python",
   "name": "sewunet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
