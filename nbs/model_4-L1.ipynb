{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import ConfigObject\n",
    "from utils import reserve_pop\n",
    "from utils import id_generator\n",
    "from utils import writer\n",
    "from utils import LibriSpeechGenerator\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from parts import VSConvBlock\n",
    "from parts import DownSamplingBlock\n",
    "from parts import UpSamplingBlock\n",
    "from parts import OutBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonConfig = {\n",
    "    \"test_platform\": False,\n",
    "    \"ds_prop\": 0.25,\n",
    "    \"sr\": 16000,\n",
    "    \"n_samples\": 65536,\n",
    "    \n",
    "    \"n_channels\": 1,\n",
    "    \"n_classes\": 1,\n",
    "    \"depth\": 5,\n",
    "    \"fsize\": 24,\n",
    "    \"moffset\": 8,\n",
    "    \n",
    "    \"batch_size\": 48,\n",
    "    \"epochs\": 25,\n",
    "    \"shuffle\": True,\n",
    "    \"num_workers\": 8,\n",
    "    \"verbose\": 100,\n",
    "\n",
    "    \"checkpoint_path\": \"../models/model_checkpoint.pt\",\n",
    "    \"model_path\": \"../models/last_model.pt\",\n",
    "\n",
    "    \"save_last_batch\": True,\n",
    "    \"writer_path\": \"../logs/\",\n",
    "    \"history_path\": \"../logs/history.json\"\n",
    "}\n",
    "\n",
    "config = ConfigObject(**jsonConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loaders\n",
    "_params = {\n",
    "    'batch_size': config.batch_size,\n",
    "    'shuffle': config.shuffle,\n",
    "    'num_workers': config.num_workers\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.load(\"../data/processed/noisy/train/x_train.pt\")\n",
    "y_train = torch.load(\"../data/processed/noisy/train/y_train.pt\")\n",
    "X_val = torch.load(\"../data/processed/noisy/val/x_val.pt\")\n",
    "y_val = torch.load(\"../data/processed/noisy/val/y_val.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Generators\n",
    "lsg = LibriSpeechGenerator(config, X_train, y_train)\n",
    "lsg_val = LibriSpeechGenerator(config, X_val, y_val)\n",
    "\n",
    "ls_generator = data.DataLoader(lsg, **_params)\n",
    "ls_val_generator = data.DataLoader(lsg_val, **_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEWUNet(nn.Module):\n",
    "    def __init__(self, config, fd=15, fu=5):\n",
    "        \"\"\"Speech Enhancenment using Wave-U-Net\"\"\"\n",
    "        super(SEWUNet, self).__init__()\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.n_channels = config.n_channels\n",
    "        self.n_classes = config.n_classes\n",
    "        self.depth = config.depth\n",
    "        self.fsize = config.fsize\n",
    "        self.moffset = config.moffset\n",
    "        self.fd = fd\n",
    "        self.fu = fu\n",
    "\n",
    "        # Generate the list of in, out channels for the encoder\n",
    "        self.enc_filters = [self.n_channels]\n",
    "        self.enc_filters += [self.fsize * i + self.moffset\n",
    "                             for i in range(1, self.depth + 1)]\n",
    "        self.n_encoder = zip(self.enc_filters, self.enc_filters[1:])\n",
    "\n",
    "        # Bottleneck block sizes\n",
    "        mid_in = self.fsize * self.depth + self.moffset\n",
    "        mid_out = self.fsize * (self.depth + 1) + self.moffset\n",
    "\n",
    "        # Generate the list of in, out channels for the decoder\n",
    "        self.out_dec = reserve_pop(self.enc_filters)\n",
    "        self.in_dec = [mid_out + self.enc_filters[-1]]\n",
    "        self.in_dec += [self.out_dec[i] + self.out_dec[i + 1]\n",
    "                        for i in range(self.depth - 1)]\n",
    "        self.n_decoder = zip(self.in_dec, self.out_dec)\n",
    "\n",
    "        # Architecture and parameters\n",
    "        self.encoder = nn.ModuleList()\n",
    "        self.decoder = nn.ModuleList()\n",
    "\n",
    "        # Build the encoder part of the U-net architecture\n",
    "        for i, (in_ch, out_ch) in enumerate(self.n_encoder):\n",
    "            self.encoder.append(DownSamplingBlock(\n",
    "                in_ch=in_ch,\n",
    "                out_ch=out_ch,\n",
    "                kernel_size=self.fd,\n",
    "                padding=self.fd // 2,\n",
    "                activation=nn.LeakyReLU(0.1))\n",
    "            )\n",
    "\n",
    "        # Bottleneck block for the U-net\n",
    "        self.mid_block = VSConvBlock(\n",
    "            in_ch=mid_in,\n",
    "            out_ch=mid_out,\n",
    "            kernel_size=self.fd,\n",
    "            padding=self.fd // 2,\n",
    "            activation=nn.LeakyReLU(0.1))\n",
    "\n",
    "        # Build the decoder part of the U-net architecture\n",
    "        for in_ch, out_ch in self.n_decoder:\n",
    "            self.decoder.append(UpSamplingBlock(\n",
    "                in_ch=in_ch,\n",
    "                out_ch=out_ch,\n",
    "                kernel_size=self.fu,\n",
    "                padding=self.fu // 2,\n",
    "                activation=nn.LeakyReLU(0.1))\n",
    "            )\n",
    "\n",
    "        # Output block\n",
    "        out_ch = self.out_dec[-1] + 1\n",
    "        self.out_block = OutBlock(\n",
    "            in_ch=out_ch,\n",
    "            out_ch=self.n_classes,\n",
    "            activation=nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\"\"\"\n",
    "        enc = []\n",
    "        net_in = copy.copy(x)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            x, xi = self.encoder[i](x)\n",
    "            enc.append(xi)\n",
    "\n",
    "        x = self.mid_block(x)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            x = self.decoder[i](x, enc.pop())\n",
    "\n",
    "        x = self.out_block(x, net_in)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "history = {'loss': [], 'SNR': [], 'val_loss': [], 'val_SNR': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SEWUNet(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomMetric():\n",
    "    \"\"\"Calculate the SNR of X and Y\"\"\"\n",
    "    def SNR(X, Y):\n",
    "        n = X.shape[2]\n",
    "        return torch.mean(10 * torch.log10(\n",
    "            (torch.norm(Y, dim=2)**2 / n) /\n",
    "            (torch.norm(X - Y, dim=2)**2 / n)\n",
    "        ))\n",
    "    return SNR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR (Validation): 10.054699897766113\n"
     ]
    }
   ],
   "source": [
    "# Build optimizer\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-6,\n",
    "    betas=(0.9, 0.999))\n",
    "\n",
    "# lr_scheduler = torch.optim.lr_scheduler(optimizer)\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)\n",
    "\n",
    "# Loss and metric\n",
    "m_loss = nn.L1Loss()\n",
    "m_snr = CustomMetric()\n",
    "\n",
    "# Print validation metric before trainer\n",
    "print(\"SNR (Validation): {}\".format(m_snr(lsg_val.X, lsg_val.y).item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy model to device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('ae_checkpoint.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1139372"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the number of trainable parameters in the model\n",
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "sum([np.prod(p.size()) for p in model_parameters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display trainning metrics\n",
    "def _display_metrics(epoch, it, steps, loss, metric):\n",
    "    print(\"Epoch [{:02d}/{:02d}]\".format(\n",
    "        epoch + 1, config.epochs), end=\", \")\n",
    "\n",
    "    print(\"Step [{:03d}/{:03d}]\".format(\n",
    "        it + 1, steps), end=\", \")\n",
    "\n",
    "    print(\"Loss: {}, SNR: {}\".format(\n",
    "        loss, metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [01/25], Step [100/601], Loss: 0.012142249383032322, SNR: 11.18836784362793\n",
      "Epoch [01/25], Step [200/601], Loss: 0.013009182177484035, SNR: 11.653497695922852\n",
      "Epoch [01/25], Step [300/601], Loss: 0.010724030435085297, SNR: 10.727903366088867\n",
      "Epoch [01/25], Step [400/601], Loss: 0.01086636632680893, SNR: 11.137312889099121\n",
      "Epoch [01/25], Step [500/601], Loss: 0.00970306433737278, SNR: 11.6358060836792\n",
      "Epoch [01/25], Step [600/601], Loss: 0.010199862532317638, SNR: 11.780844688415527\n",
      ".:. Training metrics = Loss: 0.011804764331763608, SNR: 11.208901736883371\n",
      ".:. Validation metrics = Loss: 0.010061821312104402, SNR: 11.833230389138752\n",
      "Epoch [02/25], Step [100/601], Loss: 0.012833632528781891, SNR: 11.435164451599121\n",
      "Epoch [02/25], Step [200/601], Loss: 0.011050508357584476, SNR: 10.92125129699707\n",
      "Epoch [02/25], Step [300/601], Loss: 0.01199659239500761, SNR: 10.83854866027832\n",
      "Epoch [02/25], Step [400/601], Loss: 0.012530631385743618, SNR: 11.518199920654297\n",
      "Epoch [02/25], Step [500/601], Loss: 0.009830424562096596, SNR: 12.430814743041992\n",
      "Epoch [02/25], Step [600/601], Loss: 0.010049917735159397, SNR: 12.526728630065918\n",
      ".:. Training metrics = Loss: 0.010617126742586948, SNR: 11.796720572122114\n",
      ".:. Validation metrics = Loss: 0.009102533265250727, SNR: 12.710458755240936\n",
      "Epoch [03/25], Step [100/601], Loss: 0.010831665247678757, SNR: 12.377012252807617\n",
      "Epoch [03/25], Step [200/601], Loss: 0.010076024569571018, SNR: 11.905543327331543\n",
      "Epoch [03/25], Step [300/601], Loss: 0.008515631780028343, SNR: 12.864883422851562\n",
      "Epoch [03/25], Step [400/601], Loss: 0.010588101111352444, SNR: 12.420215606689453\n",
      "Epoch [03/25], Step [500/601], Loss: 0.008443979546427727, SNR: 12.467170715332031\n",
      "Epoch [03/25], Step [600/601], Loss: 0.008837070316076279, SNR: 13.081104278564453\n",
      ".:. Training metrics = Loss: 0.009812157247306578, SNR: 12.375983113814973\n",
      ".:. Validation metrics = Loss: 0.008637989504192358, SNR: 13.053091196243297\n",
      "Epoch [04/25], Step [100/601], Loss: 0.00952125620096922, SNR: 12.757183074951172\n",
      "Epoch [04/25], Step [200/601], Loss: 0.00899217277765274, SNR: 12.348661422729492\n",
      "Epoch [04/25], Step [300/601], Loss: 0.009079164825379848, SNR: 12.92831039428711\n",
      "Epoch [04/25], Step [400/601], Loss: 0.00936589390039444, SNR: 12.951804161071777\n",
      "Epoch [04/25], Step [500/601], Loss: 0.008854462765157223, SNR: 12.504156112670898\n",
      "Epoch [04/25], Step [600/601], Loss: 0.009328949265182018, SNR: 12.548835754394531\n",
      ".:. Training metrics = Loss: 0.009346464686432862, SNR: 12.74877296986645\n",
      ".:. Validation metrics = Loss: 0.008395885594220696, SNR: 13.213154761761592\n",
      "Epoch [05/25], Step [100/601], Loss: 0.00911703985184431, SNR: 12.555440902709961\n",
      "Epoch [05/25], Step [200/601], Loss: 0.009275350719690323, SNR: 13.081903457641602\n",
      "Epoch [05/25], Step [300/601], Loss: 0.009574361145496368, SNR: 12.721636772155762\n",
      "Epoch [05/25], Step [400/601], Loss: 0.009424806572496891, SNR: 12.989768028259277\n",
      "Epoch [05/25], Step [500/601], Loss: 0.008977562189102173, SNR: 13.602778434753418\n",
      "Epoch [05/25], Step [600/601], Loss: 0.009051546454429626, SNR: 13.2027587890625\n",
      ".:. Training metrics = Loss: 0.009052650208671091, SNR: 12.994440157009464\n",
      ".:. Validation metrics = Loss: 0.008277856182485338, SNR: 13.186942706124489\n",
      "Epoch [06/25], Step [100/601], Loss: 0.008011636324226856, SNR: 13.00979995727539\n",
      "Epoch [06/25], Step [200/601], Loss: 0.008445394225418568, SNR: 12.518412590026855\n",
      "Epoch [06/25], Step [300/601], Loss: 0.009488329291343689, SNR: 12.841231346130371\n",
      "Epoch [06/25], Step [400/601], Loss: 0.00829183030873537, SNR: 13.814186096191406\n",
      "Epoch [06/25], Step [500/601], Loss: 0.010019761510193348, SNR: 13.047041893005371\n",
      "Epoch [06/25], Step [600/601], Loss: 0.008528423495590687, SNR: 13.553702354431152\n",
      ".:. Training metrics = Loss: 0.008824889678041074, SNR: 13.189750734382994\n",
      ".:. Validation metrics = Loss: 0.008124937217892461, SNR: 13.425130936936139\n"
     ]
    }
   ],
   "source": [
    "# Train the model over epochs\n",
    "steps = len(ls_generator)\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "    # training and val metrics for all data\n",
    "    loss, metric = 0.0, 0.0\n",
    "    val_loss, val_metric = 0.0, 0.0\n",
    "\n",
    "    # ======================== Training ============================= #\n",
    "    for i, (local_batch, local_labels) in enumerate(ls_generator):\n",
    "        # Transfer to Device\n",
    "        local_batch = local_batch.to(device)\n",
    "        local_labels = local_labels.to(device)\n",
    "\n",
    "        # Set gradient\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass, backward pass, optimize\n",
    "        outputs = model(local_batch)\n",
    "        loss_batch = m_loss(outputs, local_labels)\n",
    "        batch_metric = m_snr(outputs, local_labels)\n",
    "        loss_batch.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute metrics to all batch\n",
    "        loss += loss_batch.item() * len(local_batch)\n",
    "        metric += batch_metric.item() * len(local_batch)\n",
    "\n",
    "        # Print the loss every \"verbose\" batches\n",
    "        if (i + 1) % config.verbose == 0:\n",
    "            _display_metrics(epoch, i, steps,\n",
    "                loss_batch.item(), batch_metric.item())\n",
    "\n",
    "    # Compute the statistics of the last epoch and save to history\n",
    "    history['loss'].append(loss / len(lsg))\n",
    "    history['SNR'].append(metric / len(lsg))\n",
    "\n",
    "    # Checkpoint the model\n",
    "    torch.save(model.state_dict(), config.checkpoint_path)\n",
    "    \n",
    "    # Print Validation statistics\n",
    "    print(\".:. Training metrics =\", end=\" \")\n",
    "    print(\"Loss: {}, SNR: {}\".format(loss / len(lsg), metric / len(lsg)))\n",
    "    \n",
    "    # ======================= Validation ============================ #\n",
    "    with torch.no_grad():\n",
    "        for local_batch, local_labels in ls_val_generator:\n",
    "            # Transfer to device\n",
    "            local_batch = local_batch.to(device)\n",
    "            local_labels = local_labels.to(device)\n",
    "\n",
    "            # Predict, get loss and metric\n",
    "            outputs = model(local_batch)\n",
    "            val_loss += m_loss(outputs, local_labels).item() \\\n",
    "                * len(local_batch)\n",
    "\n",
    "            val_metric += m_snr(outputs, local_labels).item() \\\n",
    "                * len(local_batch)\n",
    "\n",
    "        val_loss /= len(lsg_val)\n",
    "        val_metric /= len(lsg_val)\n",
    "                \n",
    "    # Print Validation statistics\n",
    "    print(\".:. Validation metrics =\", end=\" \")\n",
    "    print(\"Loss: {}, SNR: {}\".format(val_loss, val_metric))\n",
    "\n",
    "    # Compute the metrics and loss of last batch and save to history\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_SNR'].append(val_metric)\n",
    "    lr_scheduler.step(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the last model\n",
    "torch.save(model.state_dict(), config.model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot network history\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.plot(history['loss'], label='train')\n",
    "plt.plot(history['val_loss'], label='val')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Traning history')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot network history\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.plot(history['SNR'], label='train')\n",
    "plt.plot(history['val_SNR'], label='val')\n",
    "plt.ylabel('SNR')\n",
    "plt.xlabel('Epoch')\n",
    "plt.title('Traning history')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save history to a JSON file\n",
    "with open(config.history_path, 'w') as fp:\n",
    "    json.dump(history, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.load(\"../data/processed/noisy/test/x_test.pt\")\n",
    "y_test = torch.load(\"../data/processed/noisy/test/y_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsg_test = LibriSpeechGenerator(config, X_test, y_test)\n",
    "ls_test_generator = data.DataLoader(lsg_test, **_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print validation metric before trainer\n",
    "print(\"SNR (Test): {}\".format(m_snr(lsg_test.X, lsg_test.y).item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_metric = 0.0, 0.0\n",
    "with torch.no_grad():\n",
    "    for local_batch, local_labels in ls_test_generator:\n",
    "        # Transfer to device\n",
    "        local_batch = local_batch.to(device)\n",
    "        local_labels = local_labels.to(device)\n",
    "\n",
    "        # Predict, get loss and metric\n",
    "        outputs = model(local_batch)\n",
    "        test_loss += m_loss(outputs, local_labels).item() \\\n",
    "            * len(local_batch)\n",
    "\n",
    "        test_metric += m_snr(outputs, local_labels).item() \\\n",
    "            * len(local_batch)\n",
    "        \n",
    "        writer(local_batch, local_labels,\n",
    "               outputs, config.sr, config.writer_path)\n",
    "\n",
    "    test_loss /= len(lsg_test)\n",
    "    test_metric /= len(lsg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "test_history = {\n",
    "    'SNR_ini': m_snr(lsg_test.X, lsg_test.y).item(),\n",
    "    'SNR': val_metric, \n",
    "    'loss': val_loss\n",
    "}\n",
    "\n",
    "with open(os.path.join(config.writer_path, 'test_history.json') , 'w') as fp:\n",
    "    json.dump(test_history, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
